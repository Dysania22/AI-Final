{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T22:51:24.874624Z",
     "start_time": "2024-10-16T22:51:24.859318Z"
    }
   },
   "source": [
    "import requests\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load spaCy model for advanced preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T22:51:28.259816Z",
     "start_time": "2024-10-16T22:51:25.108362Z"
    }
   },
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43men_core_web_sm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AI_Project\\.venv\\Lib\\site-packages\\spacy\\__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[0;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[0;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[0;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \n\u001B[0;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AI_Project\\.venv\\Lib\\site-packages\\spacy\\util.py:472\u001B[0m, in \u001B[0;36mload_model\u001B[1;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[0;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[0;32m    471\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[1;32m--> 472\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[1;31mOSError\u001B[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the SciBERT tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"vAAvEdRZ.JMceyc7bQU6WnoXFF6M79GPKz3I5Jo4d\"\n",
    "# API base URL for the PatentSearch API\n",
    "PATENTSEARCH_API_URL = \"https://search.patentsview.org/api/v1/patent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for advanced text preprocessing with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_spacy(text):\n",
    "    doc = nlp(text.lower())  # Process text with spaCy\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Remove stopwords, punctuation, and keep only nouns/adjectives/verbs\n",
    "        if not token.is_stop and not token.is_punct and token.pos_ in ('NOUN', 'VERB', 'ADJ'):\n",
    "            tokens.append(token.lemma_)  # Lemmatize the token (e.g., \"running\" -> \"run\")\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to query the PatentSearch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_patentsearch(query_text, num_results=10):\n",
    "    headers = {\n",
    "        'X-Api-Key': API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    # Construct the query object\n",
    "    query = {\n",
    "        \"patent_abstract\": {\"_text_any\": query_text},\n",
    "        \"patent_date\": {\"_gte\": \"2000-01-01\", \"_lte\": \"2023-12-31\"}  # Filter between a specific date range\n",
    "    }\n",
    "\n",
    "    # Specify the fields you want (patent_abstract, patent_title, patent_date, etc.)\n",
    "    fields = [\"patent_id\", \"patent_title\", \"patent_abstract\", \"patent_date\"]\n",
    "\n",
    "    # Prepare API parameters\n",
    "    params = {\n",
    "        \"q\": json.dumps(query),  # Ensure query is JSON encoded\n",
    "        \"f\": json.dumps(fields),  # Specify the fields to return\n",
    "        \"per_page\": 10,  # Number of results to return\n",
    "        \"page\": 1  # Page number (for pagination)\n",
    "    }\n",
    "\n",
    "    # Send GET request to the API\n",
    "    response = requests.get(PATENTSEARCH_API_URL, params=params, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('patents', [])\n",
    "    else:\n",
    "        print(f\"Error querying PatentSearch API: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate SciBERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n\n",
    "    # Get the embeddings from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n\n",
    "    # Use the mean of the token embeddings as the sentence embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(query_embedding, patent_embeddings):\n",
    "    similarities = cosine_similarity(query_embedding, patent_embeddings)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to rank patents based on similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_patents(query_text, patents):\n",
    "    # Preprocess the patent abstracts using spaCy\n",
    "    patent_texts = [preprocess_text_spacy(patent['patent_abstract']) for patent in patents]\n\n",
    "    # Generate embeddings for the query and patents\n",
    "    query_embedding = get_embedding(preprocess_text_spacy(query_text))\n",
    "    patent_embeddings = np.vstack([get_embedding(text) for text in patent_texts])\n\n",
    "    # Calculate similarity scores\n",
    "    similarities = calculate_similarity(query_embedding, patent_embeddings)\n\n",
    "    # Rank patents by similarity\n",
    "    ranked_patents = sorted(\n",
    "        zip(patents, similarities[0]), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "    return ranked_patents, patent_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to perform K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_patents(patent_embeddings, num_clusters=3):\n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(patent_embeddings)\n",
    "    return clusters, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_cluster(patents):\n",
    "    all_text = ' '.join([patent['patent_title'] + ' ' + patent['patent_abstract'] for patent in patents])\n",
    "    tokens = preprocess_text_spacy(all_text).split()  # Preprocess the text using spaCy\n",
    "    counter = Counter(tokens)  # Count word frequencies\n",
    "    # Get the 5 most common words\n",
    "    most_common_words = [word for word, freq in counter.most_common(5)]\n",
    "    return ', '.join(most_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to run the prior art search and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_art_search(input_patent_text, num_clusters=3):\n",
    "    # Step 1: Query the PatentSearch API to get relevant patents\n",
    "    print(\"Querying PatentSearch API for similar patents...\")\n",
    "    patents = query_patentsearch(input_patent_text, num_results=10)\n",
    "    if not patents:\n",
    "        print(\"No patents found.\")\n",
    "        return\n\n",
    "    # Step 2: Rank patents based on their similarity to the input text\n",
    "    #print(f\"{patents}\")\n",
    "    print(\"Ranking patents based on semantic similarity...\")\n",
    "    ranked_patents, patent_embeddings = rank_patents(input_patent_text, patents)\n\n",
    "    # Step 3: Select only the top 15 most similar patents\n",
    "    top_patents = ranked_patents[:15]  # Select the top 15 ranked patents\n",
    "    top_patent_embeddings = np.vstack([get_embedding(preprocess_text_spacy(patent['patent_abstract'])) for patent, _ in top_patents])\n\n",
    "    # Step 3: Display the top ranked patents\n",
    "    print(\"\\nTop 5 most similar patents:\")\n",
    "    for idx, (patent, similarity) in enumerate(ranked_patents[:5], 1):\n",
    "        print(f\"{idx}. Patent Number: {patent['patent_id']}\")\n",
    "        print(f\"   Title: {patent['patent_title']}\")\n",
    "        print(f\"   Abstract: {patent['patent_abstract']}\")\n",
    "        print(f\"   Similarity: {similarity:.4f}\")\n",
    "        print(f\"   Date: {patent['patent_date']}\")\n",
    "        print(\"\\n\")\n\n",
    "    # Step 4: Perform K-Means clustering on the patent embeddings\n",
    "    print(f\"Clustering patents into {num_clusters} clusters...\")\n",
    "    clusters, kmeans = cluster_patents(top_patent_embeddings, num_clusters)\n\n",
    "    # Step 5: Display the patents in each cluster\n",
    "    print(\"\\nPatents clustered into groups:\")\n",
    "    for cluster_id in range(num_clusters):\n",
    "        clustered_groups = [patent for i, (patent, _) in enumerate(top_patents) if clusters[i] == cluster_id]\n\n",
    "        # Step 6: Generate and print the cluster summary\n",
    "       # print(f\"\\nSummary of Cluster {cluster_id + 1}:\")\n",
    "        summary = summarize_cluster(clustered_groups)  # Generate a summary for the cluster\n",
    "        #print(f\"   Most common words: {summary}\")\n\n",
    "        # Print the cluster number\n",
    "        print(f\"\\nCluster {cluster_id + 1}: Summary - {summary}\")\n\n",
    "        # Print patents in this cluster\n",
    "        for patent in clustered_groups:\n",
    "            print(f\" - {patent['patent_id']}: {patent['patent_title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input patent text (abstract/claims)\n",
    "    input_patent_text = \"a motor positioned above the water chamber and coupled to the basket, the motor agitating the basket\"\n",
    "    # Replace with actual patent text or abstract)\n",
    "    num_clusters = 3\n",
    "    prior_art_search(input_patent_text, num_clusters=num_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
