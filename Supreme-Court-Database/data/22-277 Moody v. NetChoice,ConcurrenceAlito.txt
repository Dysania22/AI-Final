::decision_cite:: 22-277
::decision_name::  Moody v. NetChoice, LLC
::decision_year:: 2024
::opinion_author:: Alito
::opinion_type:: Concurrence
::opinion:: 

															

															SUPREME COURT OF THE UNITED STATES

															_________________

															Nos. 22–277 and 22–555

															_________________

															ASHLEY MOODY, ATTORNEY GENERAL OF
FLORIDA, et al., PETITIONERS

															22–277v.

															NETCHOICE, LLC, dba NETCHOICE, et
al.

															on writ of certiorari to the united states
court of appeals for the eleventh circuit

															

															NETCHOICE, LLC, dba NETCHOICE, et al.,
PETITIONERS

															22–555v.

															KEN PAXTON, ATTORNEY GENERAL OF
TEXAS

															on writ of certiorari to the united states
court of appeals for the fifth circuit

															[July 1, 2024]

															

															Justice Alito, with whom Justice Thomas and
Justice Gorsuch join, concurring in the judgment.

															The holding in these cases is narrow: NetChoice
failed to prove that the Florida and Texas laws they challenged are
facially unconstitutional. Everything else in the opinion of the
Court is nonbinding dicta.

															I agree with the bottom line of the majority’s
central holding. But its description of the Florida and Texas laws,
as well as the litigation that shaped the question before us,
leaves much to be desired. Its summary of our legal precedents is
incomplete. And its broader ambition of providing guidance on
whether one part of the Texas law is unconstitutional as applied to
two features of two of the many platforms that it reaches—namely,
Facebook’s News Feed and YouTube’s homepage—is unnecessary and
unjustified.

															But given the incompleteness of this record,
there is no need and no good reason to decide anything other than
the facial unconstitutionality question actually before us. After
all, we do not know how the platforms “moderate” their users’
content, much less whether they do so in an inherently expressive
way under the First Amendment. Nevertheless, the majority is
undeterred. It inexplicably singles out a few provisions and a
couple of platforms for special treatment. And it unreflectively
assumes the truth of NetChoice’s unsupported assertion that
social-media platforms—which use secret algorithms to review and
moderate an almost unimaginable quantity of data today—are just as
expressive as the newspaper editors who marked up typescripts in
blue pencil 50 years ago.

															These as-applied issues are important, and we
may have to decide them before too long. But these cases do not
provide the proper occasion to do so. For these reasons, I am
therefore compelled to provide a more complete discussion of those
matters than is customary in an opinion that concurs only in the
judgment.

															I

															As the Court has recognized, social-media
platforms have become the “modern public square.” Packingham
v. North Carolina, 582 U.S. 98, 107 (2017). In just a few
years, they have transformed the way in which millions of Americans
communicate with family and friends, perform daily chores, conduct
business, and learn about and comment on current events. The vast
majority of Americans use social media,[1] and the average person spends more than two hours a day
on various platforms.[2] Young
people now turn primarily to social media to get the news,[3] and for many of them, life without
social media is unimaginable.[4] Social media may provide many benefits—but not without
drawbacks. For example, some research suggests that social media
are having a devastating effect on many young people, leading to
depression, isolation, bullying, and intense pressure to endorse
the trend or cause of the day.[5]

															In light of these trends, platforms and
governments have implemented measures to minimize the harms unique
to the social-media context. Social-media companies have created
user guidelines establishing the kinds of content that users may
post and the consequences of violating those guidelines, which
often include removing nonconforming posts or restricting
noncompliant users’ access to a platform.

															Such enforcement decisions can sometimes have
serious consequences. Restricting access to social media can impair
users’ ability to speak to, learn from, and do business with
others. Deleting the account of an elected official or candidate
for public office may seriously impair that individual’s efforts to
reach constituents or voters, as well as the ability of voters to
make a fully informed electoral choice. And what platforms call
“content moderation” of the news or user comments on public affairs
can have a substantial effect on popular views.

															Concerned that social-media platforms could
abuse their enormous power, Florida and Texas enacted laws that
prohibit them from disfavoring particular viewpoints and speakers.
See S. B. 7072, 2021 Reg. Sess., §1(9) (Fla. 2021) (finding
that “[s]ocial media platforms have unfairly censored
. . . Floridians”); H. B. 20, 87th Leg., Called
Sess. (Tex. 2021) (prohibiting the “censorship of . . .
expression on social media platforms” in Texas). Both statutes have
a broad reach, and it is impossible to determine whether they are
unconstitutional in all their applications without surveying those
applications. The majority, however, provides only a cursory
outline of the relevant provisions of these laws and the litigation
challenging their constitutionality. To remedy this deficiency, I
will begin with a more complete summary.

															A

															1

															I start with Florida’s law, S. B. 7072,
which regulates any internet platform that does “business in the
state” and has either “annual gross revenues in excess of $100
million” or “at least 100 million monthly individual platform
participants globally.” Fla. Stat. §501.2041(1)(g) (2023). This
definition is broad. There is no dispute that it covers large
social-networking websites like Facebook, X, YouTube, and
Instagram, but it may also reach e-commerce and other
non-social-networking websites that allow users to leave reviews,
ask and answer questions, or communicate with others online. These
may include Uber, Etsy, PayPal, Yelp, Wikipedia, and Gmail. See,
e.g., Tr. of Oral Arg. in No. 22–555, pp. 54–56, 69, 76–79,
155; Brief for Wikimedia Foundation as Amicus Curiae 6;
Brief for Yelp Inc. as Amicus Curiae 4, n. 4.

															To prevent covered platforms from unfairly
treating Floridians, S. B. 7072 imposes the following
“content- moderation” and disclosure requirements:

															Content-moderation provisions. “Content
moderation” is the gentle-sounding term used by internet platforms
to denote actions they take purportedly to ensure that user-
provided content complies with their terms of service and
“community standards.” The Florida law eschews this neologism and
instead uses the old-fashioned term “censorship.” To prevent
platforms from discriminating against certain views or speakers,
that law requires each regulated platform to enforce its
“censorship . . . standards in a consistent manner among
its users on the platform.” Fla. Stat. §501.2041(2)(b). The law
defines “censorship” as any action taken to: “delete, regulate,
restrict, edit, alter, [or] inhibit” users from posting their own
content; “post an addendum to any content or material posted by a
user”; or “inhibit the ability of a user to be viewable by or to
interact with another user.” §501.2041(1)(b).

															To prevent platforms from attempting to evade
this restriction by regularly modifying their practices, the law
prohibits platforms from changing their censorship “rules, terms,
and agreements . . . more than once every 30 days.”
§501.2041(2)(c). And to give Floridians more control over how they
view content on social-media websites, the law requires each
platform to give its users the ability to “opt out” of its
content-sorting “algorithms” and instead view posts sequentially or
chronologically. §501.2041(2)(f ).[6]

															Although some platforms still have employees who
monitor and organize social-media feeds, for most platforms, “the
incredible volume of content shared each day makes human review of
each new post impossible.” Brief for Developers Alliance et al. as
Amici Curiae 4. Consequently, platforms rely heavily on
algorithms to organize and censor content. Ibid. And it is
likely that they will increasingly rely on artificial intelligence
(AI), a machine learning tool that arranges, deletes, and modifies
content and learns from its own choices.

															In addition to barring censorship, the Florida
law attempts to prevent platforms from unfairly influencing
elections or distorting public discourse. To do this, it requires
platforms to host candidates for public office and journalistic
enterprises.[7]
§§501.2041(2)(h), (j). For the same reasons, the law also prohibits
platforms from censoring posts made by or about candidates for
public office. §501.2041(2)(h).

															Disclosure provisions. S. B. 7072
requires platforms to make both general and individual disclosures
about how and when they censor the speech of Floridians. The law
requires platforms to publish their content-moderation standards
and to inform users of any changes. §§501.2041(2)(a), (c). And
whenever a platform censors a user, S. B. 7072 requires it to:
(1) notify the user of the censorship decision in writing
within seven days; (2) provide “a thorough” explanation of the
action and how the platform became aware of the affected content;
and (3) allow the user “to access or retrieve all of the
user’s information, content, material, and data for at least 60
days.” §§501.2041(2)(d), (i), (3).

															To ensure compliance with these provisions,
S. B. 7072 authorizes the Florida attorney general to bring
civil and administrative actions against noncomplying platforms.
§501.2041(5). The law allows the Florida Elections Commission to
fine platforms that fail to host candidates for public office. Fla.
Stat. §106.072(3) (2023). And the law permits aggrieved users to
sue and recover up to $100,000 for each violation of the
content-moderation and disclosure provisions, along with actual
damages, equitable relief, punitive damages, and attorney’s fees.
§501.2041(6).

															To protect platforms, the law provides that it
“may only be enforced to the extent not inconsistent with federal
law,” including §203 of the Communications Decency Act of 1996.
§501.2041(9). Section 230(c)(2)(A) of that Act shields internet
platforms from liability for voluntary, good-faith efforts to
restrict or remove content that is “obscene, lewd, lascivious,
filthy, excessively violent, harassing, or otherwise
objectionable.” 47 U. S. C. §230(c)(2)(A).

															2

															Days after S. B. 7072’s enactment,
NetChoice filed suit in federal court, alleging that the new law
violates the First Amendment in all its applications.[8] As a result, NetChoice asked the District
Court to enter a preliminary injunction against any enforcement of
any of its provisions before the law took effect.

															Florida defended the constitutionality of
S. B. 7072. It argued that the law’s prohibition of censorship
does not violate the freedom of speech because the First Amendment
permits the regulation of the conduct of entities that do not
express their own views but simply provide the means for others to
communicate. See Record in No. 4:21–CV–00220 (ND Fla.), Doc. 106,
p. 22 (citing Rumsfeld v. Forum for Academic and
Institutional Rights, Inc., 547 U.S.
47, 64 (2006) (FAIR)). And, in any event, Florida argued
that NetChoice’s facial challenge was likely to fail at the
threshold because NetChoice had not identified which of its members
were required to comply with the new law or how each of its
members’ presentation of third-party speech expressed that
platform’s own message. Record, Doc. 106, at 30, 58–59; id.,
Doc. 118, pp. 5, 24–25. Without this information, Florida
said, it could not properly respond to NetChoice’s facial claim.
Id., Doc. 122, pp. 4–5. Florida requested a “meaningful
opportunity to take discovery.” Tr. of Oral Arg. in No. 22–277,
p. 154. NetChoice objected. Record, Doc. 122.

															Despite these arguments, the District Court
enjoined S. B. 7072 in its entirety before the law could go
into effect. Florida appealed, maintaining, among other things,
that NetChoice was “unlikely to prevail on the merits of [its]
facial First Amendment challenge.” Brief for Appellants in No.
21–12355 (CA11), p. 20; Reply Brief in No. 21–12355 (CA11), p.
15.

															With just one exception, the Eleventh Circuit
affirmed. It first held that all the regulated platforms’
decisions about “whether, to what extent, and in what manner to
disseminate third-party created content to the public” were
constitutionally protected expression. NetChoice v.
Attorney Gen., Fla., 34 F. 4th 1196, 1212 (2022). Under
that framing, the court found that the moderation and individual-
disclosure provisions likely failed intermediate scrutiny,
obviating the need to determine whether strict scrutiny applied.
Id., at 1227.[9] But the
court held that the general-disclosure provisions, which require
only that platforms publish their censorship policies, met the
intermediate-scrutiny standard set forth in Zauderer v.
Office of Disciplinary Counsel of Supreme Court of Ohio,
471 U.S.
626 (1985). 34 F. 4th, at 1230. The Eleventh Circuit
therefore vacated the portion of the District Court’s order that
enjoined the enforcement of those general-disclosure provisions,
while affirming all the rest of the injunction. Id., at
1231.

															B

															1

															Around the same time as the enactment of the
Florida law, Texas adopted a similar measure, H. B. 20, which
covers “social media platform[s]” with more than 50 million monthly
users in the United States. Tex. Bus. & Com. Code Ann.
§120.002(b) (West 2023). The statute defines a “ ‘[s]ocial
media platform’ ” as an “[i]nternet website or application
that is open to the public, allows a user to create an account, and
enables users to communicate with other users for the primary
purpose of posting information, comments, messages, or images.”
§120.001(l). Unlike Florida’s broader law, however, Texas’s statute
does not cover internet-service providers, email providers, and
websites that “consis[t] primarily of news, sports, entertainment,
or other information or content that is not user generated but is
preselected by the provider.” §120.001(1)(C)(i).

															To ensure “the free exchange of ideas and
information,” H. B. 20 requires regulated platforms to abide
by the following content-moderation and disclosure requirements.
Act of Sept. 2, 2021, 87th Leg., 2d Called Sess., ch. 3.

															Content-moderation provisions. H. B.
20 prevents social-media companies from “censoring” users—that is,
acting to “block, ban, remove, deplatform, demonetize, de-boost,
restrict, deny equal access or visibility to, or otherwise
discriminate against”—based on their viewpoint or geographic
location within Texas.[10],[11],[12] Tex. Civ. Prac. & Rem. Code
Ann. §§143A.001(1), 143A.002(a)(1)–(3) (West Cum. Supp. 2023).
However, the law allows platforms to censor speech that: federal
law “specifically authorize[s]” them to censor; speech that the
platform is told sexually exploits children or survivors of sexual
abuse; speech that “directly incites criminal activity or consists
of specific threats of violence targeted against a person or group
because of race, color, disability, religion, national origin or
ancestry, age, sex or status as peace officer or judge”; and speech
that is otherwise unlawful or has been the subject of a user’s
request for removal from his or her feed or profile.
§§143A.006(a)–(b).

															Disclosure provisions. Like the Florida
law, H. B. 20 also requires platforms to make general and
individual disclosures about their censorship practices.
Specifically, the law obligates each platform to tell the public
how it “targets,” “promotes,” and “moderates” content.
§§120.051(a)(1)–(3). And whenever a platform censors a user, the
law requires it to inform the user why that was done.
§120.103(a)(1).[13]
Platforms must allow users to appeal removal decisions through “an
easily accessible complaint system;” resolve such appeals within 14
business days (unless an enumerated exception applies); and, if the
appeal is successful, provide “the reason for the reversal.”
§§120.101, 120.103(a)(2), (a)(3)(B)–(b), 120.104.

															Users may sue any platform that violates these
provisions, as may the Texas attorney general. §143A.007(d). But
unlike the Florida law, H. B. 20 authorizes only injunctive
relief. §§143A.007(a), 143 A. 008. It contains a strong
severability provision, §8(a), which reaches “every provision,
section, subsection, sentence, clause, phrase, or word in th[e]
Act, and every application of [its] provisions.”

															2

															As it did in the Florida case, NetChoice
sought a preliminary injunction in federal court, claiming that
H. B. 20 violates the First Amendment in its entirety. In
response, Texas argued that because H. B. 20 regulates
NetChoice’s members “in their operation as publicly accessible
conduits for the speech of others” rather than “as authors or
editors” of their own speech, NetChoice could not prevail. Record
in No. 1:21–CV–00840 (WD Tex.), Doc. 39, p. 23. But even if
the platforms might have the right to use algorithms to censor
their users’ speech, the State argued, the question of “what
these algorithms are doing is a critical, and so far, unexplained,
aspect of this case.” Id., at 24. This deficiency mattered,
Texas contended, because the platforms could succeed on their
facial challenge only by showing that “all algorithms used
by the Platforms are for the purposes of expressing viewpoints of
those Platforms.” Id., at 27. And because NetChoice had not
even explained what its members’ algorithms did, much less whether
they did so in an expressive way, Texas argued that NetChoice had
not shown that “all applications of H.B. 20 are unconstitutional.”
Ibid.; see also id., Doc. 53, at 13 (arguing that
NetChoice had failed to show that “H. B. 20 is . . .
unconstitutional in all its applications” because “a number” of
NetChoice’s members had conceded that the law did “not
burden or chill their speech”).

															To clarify these and other “threshold issues,”
Texas moved for expedited discovery. Id., Doc. 20, at 1. The
District Court granted Texas’s motion in part, but after one month
of discovery, it sided with NetChoice and enjoined H. B. 20 in
its entirety before it could go into effect. Texas appealed,
arguing that despite the District Court’s judgment to the contrary,
“[l]aws requiring commercial entities to neutrally host speakers
generally do not even implicate the First Amendment because they do
not regulate the host’s speech at all—they regulate its conduct.”
Brief for Appellant in No. 21–51178 (CA5), p. 16. The State also
emphasized NetChoice’s alleged failure to show that H. B. 20
was unconstitutional in even a “ ‘substantial number of its
applications,’ ” the “bare minimum” showing that NetChoice
needed to make to prevail on its facial challenge. E.g.,
Reply Brief in No. 21–51178 (CA5), p. 8 (quoting Americans for
Prosperity Foundation v. Bonta, 594 U.S. 595, 615
(2021)).

															A divided Fifth Circuit panel reversed, focusing
primarily on NetChoice’s failure to “even try to show that HB 20 is
‘unconstitutional in all of its applications.’ ” 49
F. 4th 439, 449 (2022) (quoting Washington State Grange
v. Washington State Republican Party, 552 U.S.
442, 449 (2008)). The court also accepted Texas’s argument that
H. B. 20 “does not regulate the Platforms’ speech at all”
because “the Platforms are not ‘speaking’ when they host other
people’s speech.” 49 F. 4th, at 448. Finally, the court upheld
the law’s disclosure requirements on the ground that they involve
the disclosure of the type of purely factual and uncontroversial
information that may be compelled under Zauderer. 49 F. 4th,
at 485.

															II

															NetChoice contends that the Florida and Texas
statutes facially violate the First Amendment, meaning that they
cannot be applied to anyone at any time under any circumstances
without violating the Constitution. Such challenges are strongly
disfavored. See Washington State Grange, 552 U. S., at
452. They often raise the risk of “ ‘premature
interpretatio[n] of statutes’ on the basis of factually barebones
records.” Sabri v. United States, 541 U.S.
600, 609 (2004). They clash with the principle that courts
should neither “ ‘anticipate a question of constitutional law
in advance of the necessity of deciding it’ ” nor
“ ‘formulate a rule of constitutional law broader than is
required by the precise facts to which it is to be applied.’ ”
Ashwander v. TVA, 297 U.S.
288, 346–347 (1936) (Brandeis, J., concurring). And they
“threaten to short circuit the democratic process by preventing
laws embodying the will of the people from being implemented in a
manner consistent with the Constitution.” Washington State
Grange, 552 U. S., at 451.

															Facial challenges also strain the limits of the
federal courts’ constitutional authority to decide only actual
“Cases” and “Controversies.” Art. III, §2. “[L]itigants typically
lack standing to assert the constitutional rights of third
parties.” United States v. Hansen, 599 U.S. 762, 769
(2023). But when a court holds that a law cannot be enforced
against anyone under any circumstances, it effectively grants
relief with respect to unknown parties in disputes that have not
yet materialized.

															For these reasons, we have insisted that parties
mounting facial attacks satisfy demanding requirements. In
United States v. Salerno, 481
U.S. 739, 745 (1987), we held that a facial challenger must
“establish that no set of circumstances exists under which the
[law] would be valid.” “While some Members of the Court have
criticized the Salerno formulation,” all have agreed “that a
facial challenge must fail where the statute has a “ ‘plainly
legitimate sweep.” ’ ” Washington State Grange,
552 U. S., at 449. In First Amendment cases, we have sometimes
phrased the requirement as an obligation to show that a law
“ ‘prohibits a substantial amount of protected speech’ ”
relative to its ‘ “plainly legitimate sweep.’ ”
Hansen, 599 U. S., at 770; Bonta, 594
U. S., at 615; United States v. Williams,
553 U.S.
285, 292–293 (2008).[14]

															NetChoice and the Federal Government urge us not
to apply any of these demanding tests because, they say, the States
disputed only the “threshold question” whether their laws “cover
expressive activity at all.” Tr. of Oral Arg. in No. 22–277, at 76;
see also id., at 84, 125; Tr. of Oral Arg. in No. 22–555, at
92. The Court unanimously rejects that argument—and for good
reason.

															First, the States did not “put all their
eggs in [one] basket.” Tr. of Oral Arg. in No. 22–277, at 76. To be
sure, they argued that their newly enacted laws were valid in all
their applications. Ibid. Both the Federal Government and
the States almost always defend the constitutionality of all
provisions of their laws. But Florida and Texas did not stop there.
Rather, as noted above, they went on to argue that NetChoice had
failed to make the showing required for a facial
challenge.[15] Therefore,
the record does not support NetChoice’s attempt to use “the party
presentation rules” as grounds for blocking our consideration of
the question whether it satisfied the facial constitutionality
test. Tr. of Oral Arg. in No. 22–555, at 92.

															Second, even if the States had not asked the
lower courts to reject NetChoice’s request for blanket relief, it
would have been improper for those courts to enjoin all
applications of the challenged laws unless that test was met. “It
is one thing to allow parties to forfeit claims, defenses, or lines
of argument; it would be quite another to allow parties to
stipulate or bind [a court] to the application of an incorrect
legal standard.” Gardner v. Galetka, 568 F.3d 862,
879 (CA10 2009); see also Kairys v. Southern Pines
Trucking, Inc., 75 F. 4th 153, 160 (CA3 2023) (“But
parties cannot forfeit the application of ‘controlling
law’ ”); United States v. Escobar, 866 F.3d 333,
339, n. 13 (CA5 2017) (per curiam) (“ ‘A party cannot
waive, concede, or abandon the applicable standard of
review’ ” (quoting Ward v. Stephens, 777 F.3d
250, 257, n. 3 (CA5 2015)).

															Represented by sophisticated counsel, NetChoice
made the deliberate choice to mount a facial challenge to both
laws, and in doing so, it obviously knew what it would have to show
in order to prevail. NetChoice decided to fight these laws on these
terms, and the Court properly holds it to that decision.

															III

															I therefore turn to the question whether
NetChoice established facial unconstitutionality, and I begin with
the States’ content-moderation requirements. To show that these
provisions are facially invalid, NetChoice had to demonstrate that
they lack a plainly legitimate sweep under the First Amendment. Our
precedents interpreting that Amendment provide the numerator (the
number of unconstitutional applications) and denominator (the total
number of possible applications) that NetChoice was required to
identify in order to make that showing. Estimating the numerator
requires an understanding of the First Amendment principles that
must be applied here, and I therefore provide a brief review of
those principles.

															A

															The First Amendment protects “the freedom of
speech,” and most of our cases interpreting this right have
involved government efforts to forbid, restrict, or compel a
party’s own oral or written expression. Agency for Int’l
Development v. Alliance for Open Society Int’l, Inc.,
570 U.S.
205, 213 (2013); Wooley v. Maynard, 430 U.S.
705, 714 (1977); West Virginia Bd. of Ed. v.
Barnette, 319 U.S.
624, 642 (1943). Some cases, however, have involved another
aspect of the free speech right, namely, the right to “presen[t]
. . . an edited compilation of speech generated by other
persons” for the purpose of expressing a particular message. See
Hurley v. Irish-American Gay, Lesbian and Bisexual Group
of Boston, Inc., 515 U.S.
557, 570 (1995). As used in this context, the term
“compilation” means any effort to present the expression of others
in some sort of organized package. See ibid.

															An example such as the famous Oxford Book of
English Poetry illustrates why a compilation may constitute
expression on the part of the compiler. The editors’ selection of
the poems included in this volume expresses their view about the
poets and poems that most deserve the attention of their
anticipated readers. Forcing the editors to exclude or include a
poem could alter the expression that the editors wish to
convey.

															Not all compilations, however, have this
expressive characteristic. Suppose that the head of a neighborhood
group prepares a directory consisting of contact information
submitted by all the residents who want to be listed. This
directory would not include any meaningful expression on the part
of the compiler.

															Because not all compilers express a message of
their own, not all compilations are protected by the First
Amendment. Instead, the First Amendment protects only those
compilations that are “inherently expressive” in their own right,
meaning that they select and present speech created by other
persons in order “to spread [the compiler’s] own message.”
FAIR, 547 U. S., at 66; Pacific Gas & Elec.
Co. v. Public Util. Comm’n of Cal., 475 U.S.
1, 10 (1986) (PG&E) (plurality opinion). If a
compilation is inherently expressive, then the compiler may have
the right to refuse to accommodate a particular speaker or message.
See Hurley, 515 U. S., at 573. But if a compilation is
not inherently expressive, then the government can require the
compiler to host a message or speaker because the accommodation
does not amount to compelled speech. Id., at 578–581.

															To show that a hosting requirement would compel
speech and thereby trigger First Amendment scrutiny, a claimant
must generally show three things.

															1

															First, a claimant must establish that its
practice is to exercise “editorial discretion in the selection and
presentation” of the content it hosts. Arkansas Ed. Television
Comm’n v. Forbes, 523 U.S.
666, 674 (1998); Hurley, 515 U. S., at 574;
ante, at 14. NetChoice describes this process as content
“curation.” But whatever you call it, not all compilers do this, at
least in a way that is inherently expressive. Some may serve as
“passive receptacle[s]” of third-party speech or as “dumb
pipes”[16] that merely emit
what they are fed. Such entities communicate no message of their
own, and accordingly, their conduct does not merit First Amendment
protection.[17] Miami
Herald Publishing Co. v. Tornillo, 418
U.S. 241, 258 (1974).

															Determining whether an entity should be viewed
as a “curator” or a “dumb pipe” may not always be easy because
different aspects of an entity’s operations may take different
approaches with respect to hosting third-party speech. The typical
newspaper regulates the content and presentation of articles
authored by its employees or others, PG&E, 475
U. S., at 8, but that same paper might also run nearly all the
classified advertisements it receives, regardless of their content
and without adding any expression of its own. Compare
Tornillo, 418 U.S.
241, with Pittsburgh Press Co. v. Pittsburgh Comm’n
on Human Relations, 413
U.S. 376 (1973). These differences may be significant for First
Amendment purposes.

															The same may be true for a parade organizer. For
example, the practice of a parade organizer may be to select the
groups that are admitted, but not the individuals who
are allowed to march as members of admitted groups. Hurley,
515 U. S., at 572–574. In such a case, each of these practices
would have to be analyzed separately.

															2

															Second, the host must use the compilation of
speech to express “some sort of collective point”—even if only at a
fairly abstract level. Id., at 568. Thus, a parade organizer
who claims a First Amendment right to exclude certain groups or
individuals would need to show at least that the message conveyed
by the groups or individuals who are allowed to march comport with
the parade’s theme. Id., at 560, 574. A parade comprising
“unrelated segments” that lumber along together willy-nilly would
likely not express anything at all. Id., at 576. And
although “a narrow, succinctly articulable message is not a
condition of constitutional protection,” compilations that organize
the speech of others in a non-expressive way (e.g.,
chronologically) fall “beyond the realm of expressi[on].”
Id., at 569; contra, ante, at 17–18.

															Our decision in PruneYard illustrates
this point. In that case, the Court held that a mall could be
required to host third-party speech (i.e., to admit
individuals who wanted to distribute handbills or solicit
signatures on petitions) because the mall’s admission policy did
not express any message, and because the mall was “open to the
public at large.” PruneYard Shopping Center v.
Robins, 447 U.S.
74, 83, 87–88 (1980); 303 Creative LLC v. Elenis,
600 U.S. 570, 590 (2023). In such circumstances, we held that the
First Amendment is not implicated merely because a host objects to
a particular message or viewpoint. See PG&E, 475
U. S., at 12.

															3

															Finally, a compiler must show that its “own
message [is] affected by the speech it [is] forced to accommodate.”
FAIR, 547 U. S., at 63. In core examples of expressive
compilations, such as a book containing selected articles,
chapters, stories, or poems, this requirement is easily satisfied.
But in other situations, it may be hard to identify any message
that would be affected by the inclusion of particular third-party
speech.

															Two precedents that the majority tries to
downplay, if not forget, are illustrative. The first is
PruneYard, which I have already discussed. The
PruneYard Court rejected the mall’s First Amendment claim
because “[t]he views expressed by members of the public in passing
out pamphlets or seeking signatures for a petition [were] not
likely [to] be identified with those of the owner.” 447 U. S.,
at 87. And if those who perused the handbills or petitions were not
likely to make that connection, any message that the mall owner
intended to convey would not be affected.

															The decision in FAIR rested on similar
reasoning. In that case, the Court did not dispute the proposition
that the law schools’ refusal to host military recruiters expressed
the message that the military should admit and retain gays and
lesbians. But the Court found no First Amendment violation because,
as in PruneYard, it was unlikely that the views of the
military recruiters “would be identified with” those of the schools
themselves, and consequently, hosting the military recruiters did
not “sufficiently interfere with any message of the school.” 547
U. S., at 64–65; contra, ante, at 25 (“[T]his Court has
never hinged a compiler’s First Amendment protection on the
risk of misattribution.”).[18]

															B

															A party that challenges government
interference with its curation of content cannot win without making
the three-part showing just outlined, but such a showing does not
guarantee victory. To prevail, the party must go on and show that
the challenged regulation of its curation practices violates the
applicable level of First Amendment scrutiny.

															Our decision in Turner makes that clear.
Although the television cable operators in that case made the
showing needed to trigger First Amendment scrutiny, they did not
ultimately prevail on their facial challenge to the Cable Act.
After a remand and more than 18 months of additional factual
development, the Court held that the law was adequately tailored to
serve legitimate and important government interests, including
“promoting the widespread dissemination of information from a
multiplicity of sources.” Turner Broadcasting System, Inc.
v. FCC, 520 U.S.
180, 189 (1997). Here, the States assert a similar interest in
fostering a free and open marketplace of ideas.[19]

															C

															With these standards in mind, I proceed to the
question whether the content-moderation provisions are facially
valid. For the following three reasons, NetChoice failed to meet
its burden.

															1

															First, NetChoice did not establish which
entities the statutes cover. This failure is critical because
it is “impossible to determine whether a statute reaches too far
without first knowing what the statute covers.” Williams,
553 U. S., at 293. When it sued Florida, NetChoice was
reluctant to disclose which of its members were covered by
S. B. 7072. Instead, it filed declarations revealing only that
the law reached “Etsy, Facebook, and YouTube.” Tr. of Oral Arg. in
No. 22–277, at 32. In this Court, NetChoice was a bit more
forthcoming, representing that S. B. 7072 also covers In-
stagram, X, Pinterest, Reddit, Gmail, Uber, and other e-commerce
websites. Id., at 69, 76; Brief for Respondents in No.
22–277, at 7, 38, 49.[20] But NetChoice has still not provided a complete
list.

															NetChoice was similarly reluctant to identify
its affected members in the Texas case. At first, NetChoice
“represented . . . that only Facebook, YouTube, and [X]
are affected by the Texas law.” Brief for Appellant in No. 21–51178
(CA5), at 1, n. 1. But in its brief in this Court, NetChoice
told us that H. B. 20 also regulates “some of the Internet’s
most popular websites, including Facebook, Instagram, Pinterest,
TikTok, Vimeo, X (formerly known as Twitter), and YouTube.” Brief
for Petitioners in No. 22–555, p. 1. And websites such as
Discord,[21]
Reddit,[22]
Wikipedia,[23] and
Yelp[24] have filed
amicus briefs claiming that they may be covered by both the
Texas and Florida laws.

															It is a mystery how NetChoice could expect to
prevail on a facial challenge without candidly disclosing the
platforms that it thinks the challenged laws reach or the nature of
the content moderation they practice. Without such information, we
have no way of knowing whether the laws at issue here “cover
websites that engage in primarily non- expressive conduct.” Tr. of
Oral Arg. in No. 22–277, at 34; see also id., at 126.
For example, among other things, NetChoice has not stated whether
the challenged laws reach websites like WhatsApp[25] and Gmail,[26] which carry messages instead of
curating them to create an independent speech product. Both
laws also appear to cover Reddit[27] and BeReal,[28] and websites like Parler,[29] which claim to engage in little or no content
moderation at all. And Florida’s law, which is even broader than
Texas’s, plainly applies to e-commerce platforms like Etsy that
make clear in their terms of service that they are “not a curated
marketplace.”[30]

															In First Amendment terms, this means that these
laws—in at least some of their applications—appear to regulate the
kind of “passive receptacle[s]” of third-party speech that receive
no First Amendment protection. Tornillo, 418 U. S., at
258. Given such uncertainty, it is impossible for us to determine
whether these laws have a “plainly legitimate sweep.”
Williams, 553 U. S., at 292; Washington State
Grange, 552 U. S., at 449.

															2

															Second, NetChoice has not established
what kinds of content appear on all the regulated platforms,
and we cannot determine whether these platforms create an
“inherently expressive” compilation of third-party speech until we
know what is being compiled.

															We know that social-media platforms
generally allow their users to create accounts; send direct
messages through private inboxes; post written messages, photos,
and videos; and comment on, repost, or otherwise interact with
other users’ posts. And NetChoice acknowledges in fairly general
terms that its members engage in most—though not all—of these
functions. But such generalities are insufficient.

															For one thing, the ways in which users post,
send direct messages, or interact with content may differ in
meaningful ways from platform to platform. And NetChoice’s failure
to account for these differences may be decisive. To see how,
consider X and Yelp. Both platforms allow users to post comments
and photos, but they differ in other respects.[31] X permits users to post (or “Tweet”) on a
broad range of topics because its “purpose is to serve the public
conversation,”[32] and as a
result, many elected officials use X to communicate with
constituents. Yelp, by contrast, allows users to post comments and
pictures only for the purpose of advertising local businesses or
providing “firsthand accounts” that reflect their “consumer
experience” with businesses.[33] It does not permit “rants about political ideologies,
a business’s employment practices, extraordinary circumstances, or
other matters that don’t address the core of the consumer
experience.”[34]

															As this example shows, X’s content is more
political than Yelp’s, and Yelp’s content is more commercial than
X’s. That difference may be significant for First Amendment
purposes. See Pittsburgh Press, 413
U.S. 376. But NetChoice has not developed the record on that
front. Nor has it shown what kinds of content appear across the
diverse array of regulated platforms.

															Social-media platforms are diverse, and each may
be unique in potentially significant ways. On the present record,
we are ill-equipped to account for the many platform-specific
features that allow users to do things like sell or purchase
goods,[35] live-stream
events,[36] request a
ride,[37] arrange a
date,[38] create a
discussion forum,[39] wire
money to friends,[40] play a
video game,[41] hire an
employee,[42] log a
run,[43] or agree to watch a
dog.[44] The challenged laws
may apply differently to these different functions, which may
present different First Amendment issues. A court cannot invalidate
the challenged laws if it has to speculate about their
applications.

															3

															Third, NetChoice has not established
how websites moderate content. NetChoice alleges that
“[c]overed websites” generally use algorithms to organize and
censor content appearing in “search results, comments, or in
feeds.” Brief for Petitioners in No. 22–555, at 4, 6. But at this
stage and on this record, we have no way of confirming whether all
of the regulated platforms use algorithms to organize all of their
content, much less whether these algorithms are expressive. See
Hurley, 515 U. S., at 568. Facebook and Reddit, for
instance, both allow their users to post about a wide range of
topics.[45] But while
Facebook uses algorithms to arrange and moderate its users’ posts,
Reddit asserts that its content is moderated by Reddit users, “not
by centralized algorithms.” Brief for Reddit, Inc., as Amicus
Curiae 2. If Reddit and other platforms entirely outsource
curation to others, they can hardly claim that their compilations
express their own views.

															Perhaps recognizing this, NetChoice argues in
passing that it cannot tell us how its members moderate content
because doing so would embolden “malicious actors” and divulge
“proprietary and closely held” information. E.g., Brief for
Petitioners in No. 22–555, at 11. But these harms are far from
inevitable. Various platforms already make similar disclosures—both
voluntarily and to comply with the European Union’s Digital
Services Act[46]—yet the sky
has not fallen. And on remand, NetChoice will have the opportunity
to contest whether particular disclosures are necessary and whether
any relevant materials should be filed under seal.

															Various NetChoice members already disclose in
broad strokes how they use algorithms to curate content. Many
platforms claim to use algorithms to identify and remove violent,
obscene, sexually explicit, and false posts that violate their
community guidelines. Brief for Developers Alliance et al. as
Amici Curiae 11. Some platforms—like X, for instance—say
they use algorithms, not for the purpose of removing all
nonconforming speech, but to “promot[e] counterspeech” that
“presents facts to correct misstatements” or “denounces hateful or
dangerous speech.”[47] Still
others, like Parler,[48]
Reddit,[49] and Signal
Messenger,[50] say they
engage in little or no content moderation.

															Some platforms have also disclosed that they use
algorithms to help their users find relevant content. The
e-commerce platform Etsy, for instance, uses an algorithm that
matches a user’s search terms to the “attributes” that a seller
ascribes to its wares.[51]
Etsy’s algorithm also accounts for things like the date of the
seller’s listing, the proximity of the seller and buyer, and the
quality of the seller’s customer-service ratings. Ibid.

															YouTube says it answers search queries based on
“relevance, engagement and quality”—taking into account how well a
search query matches a video title, the kinds of videos a
particular user viewed in the past, and each creator’s “expertise,
authoritativeness, and trustworthiness on a given topic.”[52]

															These disclosures suggest that platforms can say
something about their content-moderation practices without enabling
malicious actors or disclosing proprietary information. They also
suggest that not all platforms curate all third-party content in an
inherently expressive way. Without more information about how
regulated platforms moderate content, it is not possible to
determine whether these laws lack “a ‘ “plainly legitimate
sweep.” ’ ” Washington State Grange, 552
U. S., at 449.

															For all these reasons, NetChoice failed to
establish whether the content-moderation provisions violate the
First Amendment on their face.

															D

															Although the only question the Court must
decide today is whether NetChoice showed that the Florida and Texas
laws are facially unconstitutional, much of the majority opinion
addresses a different question: whether the Texas law’s
content-moderation provisions are constitutional as applied to two
features of two platforms—Facebook’s News Feed and YouTube’s
homepage. The opinion justifies this discussion on the ground that
the Fifth Circuit cannot apply the facial constitutionality test
without resolving that question, see, e.g., ante, at
13, 30, but that is not necessarily true. Especially in light of
the wide reach of the Texas law, NetChoice may still fall far short
of establishing facial unconstitutionality—even if it is assumed
for the sake of argument that the Texas law is unconstitutional as
applied to Facebook’s News Feed and YouTube’s homepage.[53]

															For this reason, the majority’s “guidance” on
this issue may well be superfluous. Yet superfluity is not its most
egregious flaw. The majority’s discussion also rests on wholly
conclusory assumptions that lack record support. For example, the
majority paints an attractive, though simplistic, picture of what
Facebook’s News Feed and YouTube’s homepage do behind the scenes.
Taking NetChoice at its word, the majority says that the platforms’
use of algorithms to enforce their community standards is
per se expressive. But the platforms have refused to
disclose how these algorithms were created and how they actually
work. And the majority fails to give any serious consideration to
key arguments pressed by the States. Most notable is the majority’s
conspicuous failure to address the States’ contention that
platforms like YouTube and Facebook—which constitute the 21st
century equivalent of the old “public square”—should be viewed as
common carriers. See Biden v. Knight First Amendment
Institute at Columbia University, 593 U. S. ___, ___
(2021) (Thomas, J., concurring) (slip op., at 6). Whether or not
the Court ultimately accepts that argument, it deserves serious
treatment.

															Instead of seriously engaging with this and
other arguments, the majority rests on NetChoice’s dubious
assertion that there is no constitutionally significant difference
between what newspaper editors did more than a half-century ago at
the time of Tornillo and what Facebook and YouTube do
today.

															Maybe that is right—but maybe it is not. Before
mechanically accepting this analogy, perhaps we should take a
closer look.

															Let’s start with size. Currently, Facebook and
YouTube each produced—on a daily basis—more than four petabytes
(4,000,000,000,000,000 bytes) of data.[54] By my calculation, that is roughly 1.3 billion
times as many bytes as there are in an issue of the New York
Times.[55]

															No human being could possibly review even a tiny
fraction of this gigantic outpouring of speech, and it is therefore
hard to see how any shared message could be discerned. And even if
someone could view all this data and find such a message, how
likely is it that the addition of a small amount of discordant
speech would change the overall message?

															Now consider how newspapers and social-media
platforms edit content. Newspaper editors are real human beings,
and when the Court decided Tornillo (the case that the
majority finds most instructive), editors assigned articles to
particular reporters, and copyeditors went over typescript with a
blue pencil. The platforms, by contrast, play no role in selecting
the billions of texts and videos that users try to convey to each
other. And the vast bulk of the “curation” and “content moderation”
carried out by platforms is not done by human beings. Instead,
algorithms remove a small fraction of nonconforming posts
post hoc and prioritize content based on factors that
the platforms have not revealed and may not even know. After all,
many of the biggest platforms are beginning to use AI algorithms to
help them moderate content. And when AI algorithms make a decision,
“even the researchers and programmers creating them don’t really
understand why the models they have built make the decisions they
make.”[56] Are such
decisions equally expressive as the decisions made by humans?
Should we at least think about this?

															Other questions abound. Maybe we should think
about the enormous power exercised by platforms like Facebook and
YouTube as a result of “network effects.” Cf. Ohio v.
American Express Co., 585 U.S. 529 (2018). And maybe we
should think about the unique ways in which social-media platforms
influence public thought. To be sure, I do not suggest that we
should decide at this time whether the Florida and Texas laws are
constitutional as applied to Facebook’s News Feed or YouTube’s
homepage. My argument is just the opposite. Such questions should
be resolved in the context of an as-applied challenge. But no
as-applied question is before us, and we do not have all the facts
that we need to tackle the extraneous matters reached by the
majority.

															Instead, when confronted with the application of
a constitutional requirement to new technology, we should proceed
with caution. While the meaning of the Constitution remains
constant, the application of enduring principles to new technology
requires an understanding of that technology and its effects.
Premature resolution of such questions creates the risk of
decisions that will quickly turn into embarrassments.

															IV

															Just as NetChoice failed to make the showing
necessary to demonstrate that the States’ content-moderation
provisions are facially unconstitutional, NetChoice’s facial
attacks on the individual-disclosure provisions also fell short.
Those provisions require platforms to explain to affected users the
basis of each content-censorship decision. Because these
regulations provide for the disclosure of “purely factual and
uncontroversial information,” they must be reviewed under
Zauderer’s framework, which requires only that such laws be
“reasonably related to the State’s interest in preventing deception
of consumers” and not “unduly burde[n]” speech. 471 U. S., at
651.[57]

															For Zauderer purposes, a law is “unduly
burdensome” if it threatens to “chil[l] protected commercial
speech.” Ibid. Here, NetChoice claims that these disclosures
have that effect and lead platforms to “conclude that the safe
course is to . . . not exercis[e] editorial discretion at
all” rather than explain why they remove “millions of posts per
day.” Brief for Respondents in No. 22–277, at 39–40 (internal
quotation marks omitted).

															Our unanimous agreement regarding NetChoice’s
failure to show that a sufficient number of its members engage in
constitutionally protected expression prevents us from accepting
NetChoice’s argument regarding these provisions. In the lower
courts, NetChoice did not even try to show how these disclosure
provisions chill each platform’s speech. Instead, NetChoice merely
identified one subset of one platform’s content that would be
affected by these laws: billions of nonconforming comments that
YouTube removes each year. 49 F. 4th, at 487; see also Brief
for Appellees in No. 21–12355 (CA11), p. 13. But if YouTube uses
automated processes to flag and remove these comments, it is not
clear why having to disclose the bases of those processes would
chill YouTube’s speech. And even if having to explain each removal
decision would unduly burden YouTube’s First Amendment rights, the
same does not necessarily follow with regard to all of NetChoice’s
members.

															NetChoice’s failure to make this broader showing
is especially problematic since NetChoice does not dispute the
States’ assertion that many platforms already provide a
notice-and-appeal process for their removal decisions. In fact,
some have even advocated for such disclosure requirements.
Before its change in ownership, the previous Chief Executive
Officer of the platform now known as X went as far as to say that
“all companies” should be required to explain censorship decisions
and “provide a straightforward process to appeal decisions made by
humans or algorithms.”[58]
Moreover, as mentioned, many platforms are already providing
similar disclosures pursuant to the European Union’s Digital
Services Act. Yet complying with that law does not appear to have
unduly burdened each platform’s speech in those countries. On
remand, the courts might consider whether compliance with EU law
chilled the platforms’ speech.

															*  *  *

															The only binding holding in these decisions is
that NetChoice has yet to prove that the Florida and Texas laws
they challenged are facially unconstitutional. Because the majority
opinion ventures far beyond the question we must decide, I concur
only in the judgment.

Notes
1
 J. Gottfried, Pew
Research Center, Americans’ Social Media Use 3 (2024). As platforms
incorporate new features and technology, the number of Americans
who use social media is expected to grow. S. Dixon, Statista,
Social Media Users in the United States 2020–2029 (Jan. 30, 2024),
https://www.statista.com/statistics/278409/number-of-social-network-users-in-the-united-states.
2
 V. Filak, Exploring Mass
Communication: Connecting With the World of Media 210
(2024).
3
 Social Media and News
Platform Fact Sheet, Pew Research Center (Nov. 15, 2023),
https://www.pewresearch.org/journalism/fact-sheet/social-media-and-news-fact-sheet.
4
 M. Anderson, M. Faverio,
& J. Gottfried, Pew Research Center, Teens, Social Media and
Technology 2023 (Dec. 11, 2023),
https://www.pewresearch.org/internet/2023/12/11/teens-social-media-and-technology-2023.
5
 Ibid.; see also J.
Twenge, J. Haidt, J. Lozano, & K. Cummins, Specification Curve
Analysis Shows That Social Media Use Is Linked to Poor Mental
Health, Especially Among Girls, 224 Acta Psychologica 1, 8–12
(2022).
6
 As relevant here, an
“algorithm” is a program that platforms use to automatically
“censor” or “moderate” content that violates their terms or
conditions, to organize the results of a search query, or to
display posts in a feed.
7
 A “journalistic
enterprise” is defined as any entity doing business in Florida
that: (1) has published more than 100,000 words online and has at
least 50,000 paid subscribers or 100,000 monthly users; (2) has
published at least 100 hours of audio or video online and has at
least 100 million annual viewers; (3) operates a cable channel that
produces more than 40 hours of content per week to at least 100,000
subscribers; or (4) operates under a Federal Communications
Commission broadcast license. Fla. Stat.
§501.2041(1)(d).
8
 NetChoice also argued
that S. B. 7072 is preempted by 47 U. S. C. §230(c)
and is unconstitutionally vague. Those arguments are not before us
because the District Court did not rule on the vagueness issue, 546
F. Supp. 3d 1082, 1095 (ND Fla. 2021), and the Eleventh
Circuit declined to reach the preemption issue, NetChoice v.
Attorney Gen., Fla., 34 F. 4th 1196, 1209
(2022).
9
 See also id., at
1214 (“unless posts and users are removed randomly, those
sorts of actions necessarily convey some sort of
message—most obviously, the platforms’ disagreement with
. . . certain content”); id., at 1223 (“S.B.
7072’s disclosure provisions implicate the First
Amendment”).
10  In
general, to “deplatform” means “to remove and ban a registered user
from a mass communication medium (such as a social networking or
blogging website).” Merriam-Webster’s Collegiate Dictionary (10th
ed. 2024), (defining “deplatform”; some punctuation omitted),
https://unabridged.merriam-webster.com/collegiate/deplatform
(unless otherwise noted, all internet sites last accessed May 22,
2024).
11  “[D]emonetization” often refers to
the act of preventing “online content from earning revenue (as from
advertisements).” Ibid. (defining“demonetize”; some
punctuation omitted),
https://unabridged.merriam-webster.com/collegiate/demonetize.
12  “Boosting on social media means
[paying] a platform to amplify . . . posts for more
reach.” C. Williams, HubSpot, Social Media Definitions: The
Ultimate Glossary of Terms You Should Know (June 23, 2023),
https://blog.hubspot.com/marketing/social-media-terms. De-boosting
thus usually refers to when platforms refuse to continue increasing
a post’s or user’s visibility to other users.
13  Texas
has represented that a brief computer-generated notification to an
affected user would satisfy the provision’s notification
requirement. Brief for Respondent in No. 22–555, p.
44.
14  At
oral argument, NetChoice represented that “it’s the plainly
legitimate sweep test, which is not synonymous with overbreadth,”
that governs these cases. See Tr. of Oral Arg. in No. 22–277, p.
70; contra, ante, at 9 (suggesting that the overbreadth
doctrine applies to all facial challenges brought under the First
Amendment, including these cases). This representation makes sense
given that the overbreadth doctrine applies only when there is “a
realistic danger that the statute itself will significantly
compromise recognized First Amendment protections of parties not
before the Court.” Members of City Council of Los Angeles v.
Taxpayers for Vincent, 466 U.S.
789, 801 (1984). And here, NetChoice appears to represent
all—or nearly all—regulated parties.
15  See
Reply Brief in No. 21–12355 (CA11), p. 15 (“Plaintiffs—in
their facial challenge—have failed to demonstrate that even a
significant subset of covered social media platforms engages in
[expressive] conduct.” See also Brief for Appellants in No.
21–12355 (CA11), p. 20 (NetChoice is “unlikely to prevail on the
merits of [its] facial First Amendment challenge”); Record in No.
4:21–CV–00220 (ND Fla.), Doc. 106, p. 30 (“Plaintiffs have not
demonstrated that their members actually [express a message],” so
there is “not a basis for sustaining Plaintiffs’ facial
constitutional challenge”); Reply Brief in No. 21–51178 (CA5), p. 8
(arguing that NetChoice failed “to show at a bare minimum that [S.
B. 20] is unconstitutional in a ‘substantial number of its
applications’ ” (quoting Americans for Prosperity
Foundation v. Bonta, 594 U.S. 595, 615 (2021))); Record
in No. 1:21–CV–00840 (WD Tex.), Doc. 39, p. 27 (because “not
all applications of H.B. 20 are unconstitutional,” “Plaintiffs’
delayed facial challenge [can]not succeed”).
16  American Broadcasting Cos. v.
Aereo, Inc., 573 U.S.
431, 458 (2014) (Scalia, J., dissenting).
17  The
majority states that it is irrelevant whether “a compiler includes
most items and excludes just a few.” Ante, at 18. That may
be true if the compiler carefully reviews, edits, and selects a
large proportion of the items it receives. But if an entity, like
some “sort of community billboard, regularly carr[ies] the messages
of third parties” instead of selecting only those that contribute
to a common theme, then this information becomes highly relevant.
PG&E, 475 U.S.
1, 23 (1986) (Marshall, J., concurring in judgment). Entities
that have assumed the role of common carriers fall into this
category, for example. And the States defend portions of their laws
on the ground that at least some social-media platforms have taken
on that role. The majority brushes aside that argument without
adequate consideration.
18  To be
sure, in Turner Broadcasting System, Inc. v.
FCC, 512 U.S.
622, 655 (1994), we held that the First Amendment applied even
though there was “little risk” of misattribution in that case. But
that is only because the claimants in that case had already shown
that the Cable Act affected the quantity or reach of the messages
that they communicated through “original programming” or television
programs produced by others. Id., at 636 (internal quotation
marks omitted). In cases not involving core examples of expressive
compilations, such as in PruneYard and FAIR, a
compiler’s First Amendment protection has very much turned on the
risk of misattribution.
19  Contrary to the majority’s
suggestion, ante, at 27, this is not the only interest that
Texas asserted. Texas has also invoked its interest in preventing
platforms from discriminating against speakers who reside in Texas
or engage in certain forms of off-platform speech. Brief for
Respondent in No. 22–555, at 15. The majority opinion does not
mention these features, much less the interests that Texas claims
they serve. Texas also asserts an interest in preventing common
carriers from engaging in “ ‘invidious discrimination in the
distribution of publicly available goods, services, and other
advantages.’ ” Id., at 18. These are “compelling state
interests of the highest order” too. Roberts v. United
States Jaycees, 468 U.S.
609, 624 (1984).
20  This
concession suggests that S. B. 7072 may “cover websites that
engage in primarily non-expressive conduct.” Tr. of Oral Arg. in
No. 22–277, at 34.
21  Brief
for Discord Inc. as Amicus Curiae 2, 21–27. “Discord is a
real time messaging service with over 150 million active monthly
users who communicate within a huge variety of interest-based
communities, or ‘servers.’ ” Id., at 1.
22  Brief
for Reddit, Inc., as Amicus Curiae 2. Reddit is an online
forum that allows its “users to establish and enforce their own
rules governing what topics are acceptable and how those topics may
be discussed . . . . The display of content on Reddit is
thus primarily driven by humans—not by centralized algorithms.”
Ibid.
23  Brief
for Wikimedia Foundation as Amicus Curiae 2.
24  Brief
for Yelp Inc. as Amicus Curiae 3–4.
25  About
WhatsApp, WhatsApp, https://whatsapp.com/about (last accessed Apr.
23, 2024).
26  Secure, Smart, and Easy To Use Email,
Gmail, https://google.com/gmail/about (last accessed Apr. 23,
2024).
27  Reddit Content Policy, Reddit,
https://www.redditinc.com/policies/content-policy (last accessed
Apr. 23, 2024) (describing Reddit as a platform that is run and
moderated by its users).
28  BeReal, which appears to have enough
monthly users to be covered by the Texas law, allows users to share
a photo with their friends once during a randomly selected 2-minute
window each day. Time To BeReal,
https://help.bereal.com/hc/en-us/articles/7350386715165--Time-to-BeReal
(last accessed Apr. 23, 2024). Twenty-four hours later, those
photos disappear. Because BeReal posts thus appear and disappear
“randomly,” even the Eleventh Circuit would agree that
BeReal likely is not an expressive compilation. 34 F. 4th, at
1214.
29  Community Guidelines, Parler,
https://www.parler.com/community-guidelines (May 31, 2024) (“We
honor the ability of all users to freely express themselves without
interference from oppressive censorship or manipulation”). Parler
probably does not have a sufficient number of monthly users to be
covered by these statutes. But it is possible that other covered
websites use a similar business model.
30  Our
House Rules, Etsy, https://etsy.com/legal/prohibited (last accessed
Apr. 23, 2024).
31  Yelp
and X are both covered by S. B. 7072 and H. B. 20. See Brief for
Yelp Inc. as Amicus Curiae 4, n. 4.
32  The X
Rules, X, https://help.x.com/en/rules-and-policies/x-rules (last
accessed Apr. 23, 2024).
33  Content Guidelines, Yelp,
https://www.yelp.com/guidelines (last accessed Apr. 23,
2024).
34  Ibid.
35  E.g., Facebook Marketplace,
Etsy.
36  E.g., X Live,
Twitch.
37  E.g., Uber, Lyft.
38  E.g., Facebook Dating,
Tinder.
39  E.g., Reddit,
Quora.
40  E.g., Meta Pay, Venmo,
PayPal.
41  E.g., Metaverse,
Discord.
42  E.g., Indeed,
LinkedIn.
43  E.g., Strava.
44  E.g., Rover.
45  Community Standards, Facebook,
https://transparency.meta.com/policies/community-standards
(“[Facebook] wants people to be able to talk openly about the
issues that matter to them, whether through written comments,
photos, music, or other artistic mediums”); Brief for Reddit, Inc.,
as Amicus Curiae 12 (“[T]he Reddit platform as a whole
accommodates a wide range of communities and modes of
discourse”).
46  Comm’n Reg. 2022/2065, Art. 17, 2022
O. J. (L. 277) 51–52. NetChoice does not dispute the States’
assertion that the regulated platforms are required to comply with
this law. Compare Brief for Petitioners in No. 22–277, p. 49, with
Reply Brief in No. 22–277, p. 24; Tr. of Oral Arg. in No.
22–555, pp. 20–21. If, on remand, the States show that the
platforms have been able to comply with this law in Europe without
having to forgo “exercising editorial discretion at all,” Brief for
Respondents in No. 22–277, p. 40, then that might help them prove
that their disclosure laws are not “unduly burdensome” under
Zauderer v. Office of Disciplinary Counsel of Supreme
Court of Ohio, 471 U.S.
626 (1985).
47  Our
Approach to Policy Development and Enforcement Philosophy, X,
http://www.help.x.com/en/rules-and-policies/enforcement-philosophy.
48  Community Guidelines, Parler,
https://www.parler.com/community-guidelines.
49  Reddit Content Policy, Reddit,
https://www.redditinc.com/policies/content-policy.
50  Signal Terms & Privacy Policy,
Signal Messenger (May 25, 2018),
https://www.signal.org/legal.
51  How
Etsy Search Works, Etsy Help Center,
https://help.etsy.com/hc/en-us/articles/115015745428–How-Etsy-Search-Works?segment=selling
(visited Apr. 9, 2024).
52  YouTube Search,
https://www.youtube.com/howyoutubeworks/product-features/search
(last accessed Apr. 23, 2024). Unlike many other platforms, YouTube
does not accept payment for better placement within organic
search
53  This
problem is even more pronounced for the Florida law, which covers
more platforms and conduct than the Texas law.
54  Breaking Down the Numbers: How Much
Data Does the World Create Daily in 2024? Edge Delta (Mar. 11,
2024), https://www.
edgedelta.com/company/blog/how-much-data-is-created-per-day.
55  The
average issue of the New York Times, excluding ads, contains about
150,000 words. A typical word consists of 10 to 20 bytes.
Therefore, the average issue of the New York Times contains around
3 million bytes.
56  T.
Xu, AI Makes Decisions We Don’t Understand—That’s a Pro-blem, (Jul.
19, 2021),
https://builtin.com/artificial-intelligence/ai-right-explanation.
57  Both
lower courts reviewed these provisions under the Zauderer
test. And in the Florida case in particular, NetChoice did not
contest—and accordingly forfeited—whether Zauderer applies
here. See Brief for Appellants in No. 21–12355 (CA11), at 21; Brief
for Appellees in No. 21–12355 (CA11), p. 44.
58  Does
Section 230’s Sweeping Immunity Enable Big Tech Bad Behavior?
Hearing before the Senate Committee on Commerce, Science, and
Transportation, 116th Cong., 2d Sess., 2 (2020) (statement of Jack
Dorsey, CEO, Twitter, Inc.).


