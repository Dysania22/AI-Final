::decision_cite:: 22-277
::decision_name::  Moody v. NetChoice, LLC
::decision_year:: 2024
::opinion_author:: Kagan
::opinion_type:: Opinion
::opinion:: 

															

															NOTICE: This opinion is subject to formal
revision before publication in the United States Reports. Readers
are requested to notify the Reporter of Decisions, Supreme Court of
the United States, Washington, D. C. 20543,
pio@supremecourt.gov, of any typographical or other formal
errors.

															SUPREME COURT OF THE UNITED STATES

															_________________

															Nos. 22–277 and 22–555

															_________________

															ASHLEY MOODY, ATTORNEY GENERAL OF
FLORIDA, et al., PETITIONERS

															22–277v.

															NETCHOICE, LLC, dba NETCHOICE, et
al.

															on writ of certiorari to the united states
court of appeals for the eleventh circuit

															

															NETCHOICE, LLC, dba NETCHOICE, et al.,
PETITIONERS

															22–555v.

															KEN PAXTON, ATTORNEY GENERAL OF
TEXAS

															on writ of certiorari to the united states
court of appeals for the fifth circuit

															[July 1, 2024]

															

															Justice Kagan delivered the opinion of the
Court.[1]*

															Not even thirty years ago, this Court felt the
need to explain to the opinion-reading public that the “Internet is
an international network of interconnected computers.” Reno
v. American Civil Liberties Union, 521
U.S. 844, 849 (1997). Things have changed since then. At the
time, only 40 million people used the internet. See id., at
850. Today, Facebook and YouTube alone have over two billion users
each. See App. in No. 22–555, p. 67a. And the public likely no
longer needs this Court to define the internet.

															These years have brought a dizzying
transformation in how people communicate, and with it a raft of
public policy issues. Social-media platforms, as well as other
websites, have gone from unheard-of to inescapable. They structure
how we relate to family and friends, as well as to businesses,
civic organizations, and governments. The novel services they offer
make our lives better, and make them worse—create unparalleled
opportunities and unprecedented dangers. The questions of whether,
when, and how to regulate online entities, and in particular the
social-media giants, are understandably on the front-burner of many
legislatures and agencies. And those government actors will
generally be better positioned than courts to respond to the
emerging challenges social-media entities pose.

															But courts still have a necessary role in
protecting those entities’ rights of speech, as courts have
historically protected traditional media’s rights. To the extent
that social-media platforms create expressive products, they
receive the First Amendment’s protection. And although these cases
are here in a preliminary posture, the current record suggests that
some platforms, in at least some functions, are indeed engaged in
expression. In constructing certain feeds, those platforms make
choices about what third-party speech to display and how to display
it. They include and exclude, organize and prioritize—and in making
millions of those decisions each day, produce their own distinctive
compilations of expression. And while much about social media is
new, the essence of that project is something this Court has seen
before. Traditional publishers and editors also select and shape
other parties’ expression into their own curated speech products.
And we have repeatedly held that laws curtailing their editorial
choices must meet the First Amendment’s requirements. The principle
does not change because the curated compilation has gone from the
physical to the virtual world. In the latter, as in the former,
government efforts to alter an edited compilation of third-party
expression are subject to judicial review for compliance with the
First Amendment.

															Today, we consider whether two state laws
regulating social-media platforms and other websites facially
violate the First Amendment. The laws, from Florida and Texas,
restrict the ability of social-media platforms to control whether
and how third-party posts are presented to other users. Or
otherwise put, the laws limit the platforms’ capacity to engage in
content moderation—to filter, prioritize, and label the varied
messages, videos, and other content their users wish to post. In
addition, though far less addressed in this Court, the laws require
a platform to provide an individualized explanation to a user if it
removes or alters her posts. NetChoice, an internet trade
association, challenged both laws on their face—as a whole, rather
than as to particular applications. The cases come to us at an
early stage, on review of preliminary injunctions. The Court of
Appeals for the Eleventh Circuit upheld such an injunction, finding
that the Florida law was not likely to survive First Amendment
review. The Court of Appeals for the Fifth Circuit reversed a
similar injunction, primarily reasoning that the Texas law does not
regulate any speech and so does not implicate the First
Amendment.

															Today, we vacate both decisions for reasons
separate from the First Amendment merits, because neither Court of
Appeals properly considered the facial nature of NetChoice’s
challenge. The courts mainly addressed what the parties had focused
on. And the parties mainly argued these cases as if the laws
applied only to the curated feeds offered by the largest and most
paradigmatic social-media platforms—as if, say, each case presented
an as-applied challenge brought by Facebook protesting its loss of
control over the content of its News Feed. But argument in this
Court revealed that the laws might apply to, and differently
affect, other kinds of websites and apps. In a facial challenge,
that could well matter, even when the challenge is brought under
the First Amendment. As explained below, the question in such a
case is whether a law’s unconstitutional applications are
substantial compared to its constitutional ones. To make that
judgment, a court must determine a law’s full set of applications,
evaluate which are constitutional and which are not, and compare
the one to the other. Neither court performed that necessary
inquiry.

															To do that right, of course, a court must
understand what kind of government actions the First Amendment
prohibits. We therefore set out the relevant constitutional
principles, and explain how one of the Courts of Appeals failed to
follow them. Contrary to what the Fifth Circuit thought, the
current record indicates that the Texas law does regulate speech
when applied in the way the parties focused on below—when applied,
that is, to prevent Facebook (or YouTube) from using its
content-moderation standards to remove, alter, organize,
prioritize, or disclaim posts in its News Feed (or homepage). The
law then prevents exactly the kind of editorial judgments this
Court has previously held to receive First Amendment protection. It
prevents a platform from compiling the third-party speech it wants
in the way it wants, and thus from offering the expressive product
that most reflects its own views and priorities. Still more, the
law—again, in that specific application—is unlikely to withstand
First Amendment scrutiny. Texas has thus far justified the law as
necessary to balance the mix of speech on Facebook’s News Feed and
similar platforms; and the record reflects that Texas officials
passed it because they thought those feeds skewed against
politically conservative voices. But this Court has many times
held, in many contexts, that it is no job for government to decide
what counts as the right balance of private expression—to “un-bias”
what it thinks biased, rather than to leave such judgments to
speakers and their audiences. That principle works for social-media
platforms as it does for others.

															In sum, there is much work to do below on both
these cases, given the facial nature of NetChoice’s challenges. But
that work must be done consistent with the First Amendment, which
does not go on leave when social media are involved.

															I

															As commonly understood, the term “social media
platforms” typically refers to websites and mobile apps that allow
users to upload content—messages, pictures, videos, and so on—to
share with others. Those viewing the content can then react to it,
comment on it, or share it themselves. The biggest social-media
companies—entities like Facebook and YouTube—host a staggering
amount of content. Facebook users, for example, share more than 100
billion messages every day. See App. in No. 22–555, at 67a. And
YouTube sees more than 500 hours of video uploaded every minute.
See ibid.

															In the face of that deluge, the major platforms
cull and organize uploaded posts in a variety of ways. A user does
not see everything—even everything from the people she follows—in
reverse-chronological order. The platforms will have removed some
content entirely; ranked or otherwise prioritized what remains; and
sometimes added warnings or labels. Of particular relevance here,
Facebook and YouTube make some of those decisions in conformity
with content-moderation policies they call Community Standards and
Community Guidelines. Those rules list the subjects or messages the
platform prohibits or discourages—say, pornography, hate speech, or
misinformation on select topics. The rules thus lead Facebook and
YouTube to remove, disfavor, or label various posts based on their
content.

															In 2021, Florida and Texas enacted statutes
regulating internet platforms, including the large social-media
companies just mentioned. The States’ laws differ in the entities
they cover and the activities they limit. But both contain
content-moderation provisions, restricting covered platforms’
choices about whether and how to display user- generated content to
the public. And both include individualized-explanation provisions,
requiring platforms to give reasons for particular
content-moderation choices.

															Florida’s law regulates “social media
platforms,” as defined expansively, that have annual gross revenue
of over $100 million or more than 100 million monthly active users.
Fla. Stat. §501.2041(1)(g) (2023).[2] The statute restricts varied ways of “censor[ing]” or
otherwise disfavoring posts—including deleting, altering, labeling,
or deprioritizing them—based on their content or source.
§501.2041(1)(b). For example, the law prohibits a platform from
taking those actions against “a journalistic enterprise based on
the content of its publication or broadcast.” §501.2041(2)(j).
Similarly, the law prevents deprioritizing posts by or about
political candidates. See §501.2041(2)(h). And the law requires
platforms to apply their content-moderation practices to users “in
a consistent manner.” §501.2041(2)(b).

															In addition, the Florida law mandates that a
platform provide an explanation to a user any time it removes or
alters any of her posts. See §501.2041(2)(d)(1). The requisite
notice must be delivered within seven days, and contain both a
“thorough rationale” for the action and an account of how the
platform became aware of the targeted material. §501.2041(3).

															The Texas law regulates any social-media
platform, having over 50 million monthly active users, that allows
its users “to communicate with other users for the primary purpose
of posting information, comments, messages, or images.” Tex. Bus.
& Com. Code Ann. §§120.001(1), 120.002(b) (West Cum. Supp.
2023).[3] With several
exceptions, the statute prevents platforms from “censor[ing]” a
user or a user’s expression based on viewpoint. Tex. Civ. Prac.
& Rem. Code Ann. §§143A.002(a), 143 A. 006 (West Cum. Supp.
2023). That ban on “censor[ing]” covers any action to “block, ban,
remove, deplatform, demonetize, de-boost, restrict, deny equal
access or visibility to, or otherwise discriminate against
expression.” §143A.001(1). The statute also requires that
“concurrently with the removal” of user content, the platform shall
“notify the user” and “explain the reason the content was removed.”
§120.103(a)(1). The user gets a right of appeal, and the platform
must address an appeal within 14 days. See §§120.103(a)(2),
120.104.

															Soon after Florida and Texas enacted those
statutes, NetChoice LLC and the Computer & Communications
Industry Association (collectively, NetChoice)—trade associations
whose members include Facebook and YouTube—brought facial First
Amendment challenges against the two laws. District courts in both
States entered preliminary injunctions, halting the laws’
enforcement. See 546 F. Supp. 3d 1082, 1096 (ND Fla. 2021);
573 F. Supp. 3d 1092, 1117 (WD Tex. 2021). Each court held
that the suit before it is likely to succeed because the statute
infringes on the constitutionally protected “editorial judgment” of
NetChoice’s members about what material they will display. See 546
F. Supp. 3d, at 1090; 573 F. Supp. 3d, at 1107.

															The Eleventh Circuit upheld the injunction of
Florida’s law, as to all provisions relevant here. The court held
that the State’s restrictions on content moderation trigger First
Amendment scrutiny under this Court’s cases protecting “editorial
discretion.” 34 F. 4th 1196, 1209, 1216 (2022). When a
social-media platform “removes or deprioritizes a user or post,”
the court explained, it makes a “judgment rooted in the platform’s
own views about the sorts of content and viewpoints that are
valuable and appropriate for dissemination.” Id., at 1210.
The court concluded that the content-moderation provisions are
unlikely to survive “intermediate—let alone strict—scrutiny,”
because a State has no legitimate interest in counteracting
“private ‘censorship’ ” by “tilt[ing] public debate in a
preferred direction.” Id., at 1227–1228. Similarly, the
Eleventh Circuit thought the statute’s individualized-explanation
requirements likely to fall. Applying the standard from
Zauderer v. Office of Disciplinary Counsel of Supreme
Court of Ohio, 471 U.S.
626 (1985), the court held that the obligation to explain
“millions of [decisions] per day” is “unduly burdensome and likely
to chill platforms’ protected speech.” 34 F. 4th, at 1230.

															The Fifth Circuit disagreed across the board,
and so reversed the preliminary injunction before it. In that
court’s view, the platforms’ content-moderation activities are “not
speech” at all, and so do not implicate the First Amendment. 49
F. 4th 439, 466, 494 (2022). But even if those activities were
expressive, the court continued, the State could regulate them to
advance its interest in “protecting a diversity of ideas.”
Id., at 482 (emphasis deleted). The court further held that
the statute’s individualized- explanation provisions would likely
survive, again even assuming that the platforms were engaged in
speech. Those requirements, the court maintained, are not unduly
burdensome under Zauderer because the platforms needed only
to “scale up” a “complaint-and-appeal process” they already used.
49 F. 4th, at 487.

															We granted certiorari to resolve the split
between the Fifth and Eleventh Circuits. 600 U. S. ___
(2023).

															II

															NetChoice chose to litigate these cases as
facial challenges, and that decision comes at a cost. For a host of
good reasons, courts usually handle constitutional claims case by
case, not en masse. See Washington State Grange v.
Washington State Republican Party, 552
U.S. 442, 450–451 (2008). “Claims of facial invalidity often
rest on speculation” about the law’s coverage and its future
enforcement. Id., at 450. And “facial challenges threaten to
short circuit the democratic process” by preventing duly enacted
laws from being implemented in constitutional ways. Id., at
451. This Court has therefore made facial challenges hard to
win.

															That is true even when a facial suit is based on
the First Amendment, although then a different standard applies. In
other cases, a plaintiff cannot succeed on a facial challenge
unless he “establish[es] that no set of circumstances exists under
which the [law] would be valid,” or he shows that the law lacks a
“plainly legitimate sweep.” United States v. Salerno,
481 U.S.
739, 745 (1987); Washington State Grange, 552
U. S., at 449. In First Amendment cases, however, this Court
has lowered that very high bar. To “provide[ ] breathing room
for free expression,” we have substituted a less demanding though
still rigorous standard. United States v. Hansen, 599
U.S. 762, 769 (2023). The question is whether “a substantial number
of [the law’s] applications are unconstitutional, judged in
relation to the statute’s plainly legitimate sweep.” Americans
for Prosperity Foundation v. Bonta, 594 U.S. 595, 615
(2021); see Hansen, 599 U. S., at 770 (likewise asking
whether the law “prohibits a substantial amount of protected speech
relative to its plainly legitimate sweep”). So in this singular
context, even a law with “a plainly legitimate sweep” may be struck
down in its entirety. But that is so only if the law’s
unconstitutional applications substantially outweigh its
constitutional ones.

															So far in these cases, no one has paid much
attention to that issue. In the lower courts, NetChoice and the
States alike treated the laws as having certain heartland
applications, and mostly confined their battle to that terrain.
More specifically, the focus was on how the laws applied to the
content-moderation practices that giant social-media platforms use
on their best-known services to filter, alter, or label their
users’ posts. Or more specifically still, the focus was on how the
laws applied to Facebook’s News Feed and YouTube’s homepage.
Reflecting the parties’ arguments, the Eleventh and Fifth Circuits
also mostly confined their analysis in that way. See 34
F. 4th, at 1210, 1213 (considering “platforms like Facebook,
Twitter, YouTube, and TikTok” and content moderation in “viewers’
feeds”); 49 F. 4th, at 445, 460, 478, 492 (considering
platforms “such as Facebook, Twitter, and YouTube” and referencing
users’ feeds); see also id., at 501 (Southwick, J.,
concurring in part and dissenting in part) (analyzing a curated
feed). On their way to opposing conclusions, they concentrated on
the same issue: whether a state law can regulate the
content-moderation practices used in Facebook’s News Feed (or near
equivalents). They did not address the full range of activities the
laws cover, and measure the constitutional against the
unconstitutional applications. In short, they treated these cases
more like as-applied claims than like facial ones.

															The first step in the proper facial analysis is
to assess the state laws’ scope. What activities, by what actors,
do the laws prohibit or otherwise regulate? The laws of course
differ one from the other. But both, at least on their face, appear
to apply beyond Facebook’s News Feed and its ilk. Members of this
Court asked some of the relevant questions at oral argument.
Starting with Facebook and the other giants: To what extent, if at
all, do the laws affect their other services, like direct messaging
or events management? See Tr. of Oral Arg. in No. 22–555,
pp. 62–63; Tr. of Oral Arg. in No. 22–277, pp. 24–25;
App. in No. 22–277, pp. 129, 159. And beyond those
social-media entities, what do the laws have to say, if anything,
about how an email provider like Gmail filters incoming messages,
how an online marketplace like Etsy displays customer reviews, how
a payment service like Venmo manages friends’ financial exchanges,
or how a ride-sharing service like Uber runs? See Tr. of Oral Arg.
in No. 22–277, at 74–79, 95–98; see also id., at 153
(Solicitor General) (“I have some sympathy [for the Court] here. In
preparation for this argument, I’ve been working with my team to
say, does this even cover direct messaging? Does this even cover
Gmail?”). Those are examples only. The online world is variegated
and complex, encompassing an ever-growing number of apps, services,
functionalities, and methods for communication and connection. Each
might (or might not) have to change because of the provisions, as
to either content moderation or individualized explanation, in
Florida’s or Texas’s law. Before a court can do anything else with
these facial challenges, it must address that set of issues—in
short, must “determine what [the law] covers.” Hansen, 599
U. S., at 770.

															The next order of business is to decide which of
the laws’ applications violate the First Amendment, and to measure
them against the rest. For the content-moderation provisions, that
means asking, as to every covered platform or function, whether
there is an intrusion on protected editorial discretion. See
infra, at 13–19. And for the individualized-explanation
provisions, it means asking, again as to each thing covered,
whether the required disclosures unduly burden expression. See
Zauderer, 471 U. S., at 651. Even on a preliminary
record, it is not hard to see how the answers might differ as
between regulation of Facebook’s News Feed (considered in the
courts below) and, say, its direct messaging service (not so
considered). Curating a feed and transmitting direct messages, one
might think, involve different levels of editorial choice, so that
the one creates an expressive product and the other does not. If
so, regulation of those diverse activities could well fall on
different sides of the constitutional line. To decide the facial
challenges here, the courts below must explore the laws’ full range
of applications—the constitutionally impermissible and permissible
both—and compare the two sets. Maybe the parties treated the
content-moderation choices reflected in Facebook’s News Feed and
YouTube’s homepage as the laws’ heartland applications because they
are the principal things regulated, and should have just
that weight in the facial analysis. Or maybe not: Maybe the
parties’ focus had all to do with litigation strategy, and there is
a sphere of other applications—and constitutional ones—that would
prevent the laws’ facial invalidation.

															The problem for this Court is that it cannot
undertake the needed inquiries. “[W]e are a court of review, not of
first view.” Cutter v. Wilkinson, 544 U.S.
709, 718, n. 7 (2005). Neither the Eleventh Circuit nor
the Fifth Circuit performed the facial analysis in the way just
described. And even were we to ignore the value of other courts
going first, we could not proceed very far. The parties have not
briefed the critical issues here, and the record is underdeveloped.
So we vacate the decisions below and remand these cases. That will
enable the lower courts to consider the scope of the laws’
applications, and weigh the unconstitutional as against the
constitutional ones.

															III

															But it is necessary to say more about how the
First Amendment relates to the laws’ content-moderation provisions,
to ensure that the facial analysis proceeds on the right path in
the courts below. That need is especially stark for the Fifth
Circuit. Recall that it held that the content choices the major
platforms make for their main feeds are “not speech” at all, so
States may regulate them free of the First Amendment’s restraints.
49 F. 4th, at 494; see supra, at 8. And even if those
activities were expressive, the court held, Texas’s interest in
better balancing the marketplace of ideas would satisfy First
Amendment scrutiny. See 49 F. 4th, at 482. If we said nothing
about those views, the court presumably would repeat them when it
next considers NetChoice’s challenge. It would thus find that
significant applications of the Texas law—and so significant inputs
into the appropriate facial analysis—raise no First Amendment
difficulties. But that conclusion would rest on a serious
misunderstanding of First Amendment precedent and principle. The
Fifth Circuit was wrong in concluding that Texas’s restrictions on
the platforms’ selection, ordering, and labeling of third-party
posts do not interfere with expression. And the court was wrong to
treat as valid Texas’s interest in changing the content of the
platforms’ feeds. Explaining why that is so will prevent the Fifth
Circuit from repeating its errors as to Facebook’s and YouTube’s
main feeds. (And our analysis of Texas’s law may also aid the
Eleventh Circuit, which saw the First Amendment issues much as we
do, when next considering NetChoice’s facial challenge.) But a
caveat: Nothing said here addresses any of the laws’ other
applications, which may or may not share the First Amendment
problems described below.[4]

															A

															Despite the relative novelty of the technology
before us, the main problem in this case—and the inquiry it calls
for—is not new. At bottom, Texas’s law requires the platforms to
carry and promote user speech that they would rather discard or
downplay. The platforms object that the law thus forces them to
alter the content of their expression—a particular edited
compilation of third-party speech. See Brief for NetChoice in No.
22–555, pp. 18–34. That controversy sounds a familiar note. We
have repeatedly faced the question whether ordering a party to
provide a forum for someone else’s views implicates the First
Amendment. And we have repeatedly held that it does so if, though
only if, the regulated party is engaged in its own expressive
activity, which the mandated access would alter or disrupt. So too
we have held, when applying that principle, that expressive
activity includes presenting a curated compilation of speech
originally created by others. A review of the relevant precedents
will help resolve the question here.

															The seminal case is Miami Herald Publishing
Co. v. Tornillo, 418 U.S.
241 (1974). There, a Florida law required a newspaper to give a
political candidate a right to reply when it published “criticism
and attacks on his record.” Id., at 243. The Court held the
law to violate the First Amendment because it interfered with the
newspaper’s “exercise of editorial control and judgment.”
Id., at 258. Forcing the paper to print what “it would not
otherwise print,” the Court explained, “intru[ded] into the
function of editors.” Id., at 256, 258. For that function
was, first and foremost, to make decisions about the “content of
the paper” and “[t]he choice of material to go into” it.
Id., at 258. In protecting that right of editorial control,
the Court recognized a possible downside. It noted the access
advocates’ view (similar to the States’ view here) that “modern
media empires” had gained ever greater capacity to “shape” and even
“manipulate popular opinion.” Id., at 249–250. And the Court
expressed some sympathy with that diagnosis. See id., at
254. But the cure proposed, it concluded, collided with the First
Amendment’s antipathy to state manipulation of the speech
market. Florida, the Court explained, could not substitute
“governmental regulation” for the “crucial process” of editorial
choice. Id., at 258.

															Next up was Pacific Gas & Elec. Co.
v. Public Util. Comm’n of Cal., 475 U.S.
1 (1986) (PG&E), which the Court thought to follow
naturally from Tornillo. See 475 U. S., at 9–12
(plurality opinion); id., at 21 (Burger, C. J.,
concurring). A private utility in California regularly put a
newsletter in its billing envelopes expressing its views of energy
policy. The State directed it to include as well material from a
consumer-advocacy group giving a different perspective. The utility
objected, and the Court held again that the interest in “offer[ing]
the public a greater variety of views” could not justify the
regulation. Id., at 12. California was compelling the
utility (as Florida had compelled a newspaper) “to carry speech
with which it disagreed” and thus to “alter its own message.”
Id., at 11, n. 7, 16.

															In Turner Broadcasting System, Inc. v.
FCC, 512 U.S.
622 (1994) (Turner I ), the Court further
underscored the constitutional protection given to editorial
choice. At issue were federal “must-carry” rules, requiring cable
operators to allocate some of their channels to local broadcast
stations. The Court had no doubt that the First Amendment was
implicated, because the operators were engaging in expressive
activity. They were, the Court explained, “exercising editorial
discretion over which stations or programs to include in [their]
repertoire.” Id., at 636. And the rules “interfere[d]” with
that discretion by forcing the operators to carry stations they
would not otherwise have chosen. Id., at 643–644. In a later
decision, the Court ruled that the regulation survived First
Amendment review because it was necessary to prevent the demise of
local broadcasting. See Turner Broadcasting System, Inc. v.
FCC, 520 U.S.
180, 185, 189–190 (1997) (Turner II ); see
infra, at 28, n. 10. But for purposes of today’s cases,
the takeaway of Turner is this holding: A private party’s
collection of third-party content into a single speech product (the
operators’ “repertoire” of programming) is itself expressive, and
intrusion into that activity must be specially justified under the
First Amendment.

															The capstone of those precedents came in
Hurley v. Irish-American Gay, Lesbian and Bisexual Group
of Boston, Inc., 515 U.S.
557 (1995), when the Court considered (of all things) a parade.
The question was whether Massachusetts could require the organizers
of a St. Patrick’s Day parade to admit as a participant a gay and
lesbian group seeking to convey a message of “pride.” Id.,
at 561. The Court held unanimously that the First Amendment
precluded that compulsion. The “selection of contingents to make a
parade,” it explained, is entitled to First Amendment protection,
no less than a newspaper’s “presentation of an edited compilation
of [other persons’] speech.” Id., at 570 (citing
Tornillo, 418 U. S., at 258). And that meant the State
could not tell the parade organizers whom to include. Because
“every participating unit affects the message,” said the Court,
ordering the group’s admittance would “alter the expressive content
of the[ ] parade.” Hurley, 515 U. S., at 572–573.
The parade’s organizers had “decided to exclude a message [they]
did not like from the communication [they] chose to make,” and that
was their decision alone. Id., at 574.

															On two other occasions, the Court distinguished
Tornillo and its progeny for the flip-side reason—because in
those cases the compelled access did not affect the
complaining party’s own expression. First, in PruneYard Shopping
Center v. Robins, 447 U.S.
74 (1980), the Court rejected a shopping mall’s First Amendment
challenge to a California law requiring it to allow members of the
public to distribute handbills on its property. The mall owner did
not claim that he (or the mall) was engaged in any expressive
activity. Indeed, as the PG&E Court later noted, he “did
not even allege that he objected to the content of the pamphlets”
passed out at the mall. 475 U. S., at 12. Similarly, in
Rumsfeld v. Forum for Academic and Institutional Rights,
Inc., 547 U.S.
47 (2006) (FAIR), the Court reiterated that a First
Amendment claim will not succeed when the entity objecting to
hosting third-party speech is not itself engaged in expression. The
statute at issue required law schools to allow the military to
participate in on-campus recruiting. The Court held that the
schools had no First Amendment right to exclude the military based
on its hiring policies, because the schools “are not speaking when
they host interviews.” Id., at 64. Or stated again, with
reference to the just-described precedents: Because a “law school’s
recruiting services lack the expressive quality of a parade, a
newsletter, or the editorial page of a newspaper,” the required
“accommodation of a military recruiter[ ]” did not “interfere
with any message of the school.” Ibid.

															That is a slew of individual cases, so consider
three general points to wrap up. Not coincidentally, they will
figure in the upcoming discussion of the First Amendment problems
the statutes at issue here likely present as to Facebook’s News
Feed and similar products.

															First, the First Amendment offers protection
when an entity engaging in expressive activity, including compiling
and curating others’ speech, is directed to accommodate messages it
would prefer to exclude. “[T]he editorial function itself is an
aspect of speech.” Denver Area Ed. Telecommunications
Consortium, Inc. v. FCC, 518 U.S.
727, 737 (1996) (plurality opinion). Or said just a bit
differently: An entity “exercis[ing] editorial discretion in the
selection and presentation” of content is “engage[d] in speech
activity.” Arkansas Ed. Television Comm’n v. Forbes,
523 U.S.
666, 674 (1998). And that is as true when the content comes
from third parties as when it does not. (Again, think of a
newspaper opinion page or, if you prefer, a parade.) Deciding on
the third-party speech that will be included in or excluded from a
compilation—and then organizing and presenting the included
items—is expressive activity of its own. And that activity results
in a distinctive expressive product. When the government interferes
with such editorial choices—say, by ordering the excluded to be
included—it alters the content of the compilation. (It creates a
different opinion page or parade, bearing a different message.) And
in so doing—in overriding a private party’s expressive choices—the
government confronts the First Amendment.[5]

															Second, none of that changes just because a
compiler includes most items and excludes just a few. That was the
situation in Hurley. The St. Patrick’s Day parade at issue
there was “eclectic”: It included a “wide variety of patriotic,
commercial, political, moral, artistic, religious, athletic, public
service, trade union, and eleemosynary themes, as well as
conflicting messages.” 515 U. S., at 562. Or otherwise said,
the organizers were “rather lenient in admitting participants.”
Id., at 569. No matter. A “narrow, succinctly articulable
message is not a condition of constitutional protection.”
Ibid. It “is enough” for a compiler to exclude the handful
of messages it most “disfavor[s].” Id., at 574. Suppose, for
example, that the newspaper in Tornillo had granted a right
of reply to all but one candidate. It would have made no
difference; the Florida statute still could not have altered the
paper’s policy. Indeed, that kind of focused editorial choice packs
a peculiarly powerful expressive punch.

															Third, the government cannot get its way just by
asserting an interest in improving, or better balancing, the
marketplace of ideas. Of course, it is critically important to have
a well-functioning sphere of expression, in which citizens have
access to information from many sources. That is the whole project
of the First Amendment. And the government can take varied
measures, like enforcing competition laws, to protect that access.
Cf., e.g., Turner I, 512 U. S., at 647
(protecting local broadcasting); Hurley, 515 U. S., at
577 (discussing Turner I ). But in case after case, the
Court has barred the government from forcing a private speaker to
present views it wished to spurn in order to rejigger the
expressive realm. The regulations in Tornillo,
PG&E, and Hurley all were thought to promote
greater diversity of expression. See supra, at 14–16. They
also were thought to counteract advantages some private parties
possessed in controlling “enviable vehicle[s]” for speech.
Hurley, 515 U. S., at 577. Indeed, the Tornillo
Court devoted six pages of its opinion to recounting a critique of
the then-current media environment—in particular, the
disproportionate “influen[ce]” of a few speakers—similar to one
heard today (except about different entities). 418 U. S., at
249; see id., at 248–254; supra, at 14–15. It made no
difference. However imperfect the private marketplace of ideas,
here was a worse proposal—the government itself deciding when
speech was imbalanced, and then coercing speakers to provide more
of some views or less of others.

															B

															“[W]hatever the challenges of applying the
Constitution to ever-advancing technology, the basic principles” of
the First Amendment “do not vary.” Brown v. Entertainment
Merchants Assn., 564 U.S.
786, 790 (2011). New communications media differ from old ones
in a host of ways: No one thinks Facebook’s News Feed much
resembles an insert put in a billing envelope. And similarly,
today’s social media pose dangers not seen earlier: No one ever
feared the effects of newspaper opinion pages on adolescents’
mental health. But analogies to old media, even if imperfect, can
be useful. And better still as guides to decision are settled
principles about freedom of expression, including the ones just
described. Those principles have served the Nation well over many
years, even as one communications method has given way to another.
And they have much to say about the laws at issue here. These
cases, to be sure, are at an early stage; the record is incomplete
even as to the major social-media platforms’ main feeds, much less
the other applications that must now be considered. See
supra, at 12. But in reviewing the District Court’s
preliminary injunction, the Fifth Circuit got its
likelihood-of-success finding wrong. Texas is not likely to succeed
in enforcing its law against the platforms’ application of their
content-moderation policies to the feeds that were the focus of the
proceedings below. And that is because of the core teaching
elaborated in the above-summarized decisions: The government may
not, in supposed pursuit of better expressive balance, alter a
private speaker’s own editorial choices about the mix of speech it
wants to convey.

															Most readers are likely familiar with Facebook’s
News Feed or YouTube’s homepage; assuming so, feel free to skip
this paragraph (and maybe a couple more). For the uninitiated,
though, each of those feeds presents a user with a continually
updating stream of other users’ posts. For Facebook’s News Feed,
any user may upload a message, whether verbal or visual, with
content running the gamut from “vacation pictures from friends” to
“articles from local or national news outlets.” App. in No. 22–555,
at 139a. And whenever a user signs on, Facebook delivers a
personalized collection of those stories. Similarly for YouTube.
Its users upload all manner of videos. And any person opening the
website or mobile app receives an individualized list of video
recommendations.

															The key to the scheme is prioritization of
content, achieved through the use of algorithms. Of the billions of
posts or videos (plus advertisements) that could wind up on a
user’s customized feed or recommendations list, only the tiniest
fraction do. The selection and ranking is most often based on a
user’s expressed interests and past activities. But it may also be
based on more general features of the communication or its creator.
Facebook’s Community Standards and YouTube’s Community Guidelines
detail the messages and videos that the platforms disfavor. The
platforms write algorithms to implement those standards—for
example, to prefer content deemed particularly trustworthy or to
suppress content viewed as deceptive (like videos promoting
“conspiracy theor[ies]”). Id., at 113a.

															Beyond rankings lie labels. The platforms may
attach “warning[s], disclaimers, or general commentary”—for
example, informing users that certain content has “not been
verified by official sources.” Id., at 75a. Likewise, they
may use “information panels” to give users “context on content
relating to topics and news prone to misinformation, as well as
context about who submitted the content.” Id., at 114a. So,
for example, YouTube identifies content submitted by
state-supported media channels, including those funded by the
Russian Government. See id., at 76a.

															But sometimes, the platforms decide, providing
more information is not enough; instead, removing a post is the
right course. The platforms’ content-moderation policies also say
when that is so. Facebook’s Standards, for example, proscribe
posts—with exceptions for “newsworth[iness]” and other “public
interest value”—in categories and subcategories including: Violence
and Criminal Behavior (e.g., violence and incitement,
coordinating harm and publicizing crime, fraud and deception);
Safety (e.g., suicide and self-injury, sexual exploitation,
bullying and harassment); Objectionable Content (e.g., hate
speech, violent and graphic content); Integrity and Authenticity
(e.g., false news, manipulated media). Id., at
412a–415a, 441a–442a. YouTube’s Guidelines similarly target videos
falling within categories like: hate speech, violent or graphic
content, child safety, and misinformation (including about
elections and vaccines). See id., at 430a–432a. The
platforms thus unabashedly control the content that will appear to
users, exercising authority to remove, label or demote messages
they disfavor.[6]

															Except that Texas’s law limits their power to do
so. As noted earlier, the law’s central provision prohibits the
large social-media platforms (and maybe other
entities [7]) from
“censor[ing]” a “user’s expression” based on its “viewpoint.”
§143A.002(a)(2); see supra, at 7. The law defines
“expression” broadly, thus including pretty much anything that
might be posted. See §143A.001(2). And it defines “censor” to mean
“block, ban, remove, deplatform, demonetize, de-boost, restrict,
deny equal access or visibility to, or otherwise discriminate
against expression.” §143A.001(1).[8] That is a long list of verbs, but it comes down to
this: The platforms cannot do any of the things they typically do
(on their main feeds) to posts they disapprove—cannot demote,
label, or remove them—whenever the action is based on the post’s
viewpoint.[9] And what does
that “based on viewpoint” requirement entail? Doubtless some of the
platforms’ content-moderation practices are based on
characteristics of speech other than viewpoint (e.g., on
subject matter). But if Texas’s law is enforced, the platforms
could not—as they in fact do now—disfavor posts because they:

															support Nazi ideology;

															advocate for terrorism;

															espouse racism, Islamophobia, or
anti-Semitism;

															glorify rape or other gender-based
violence;

															encourage teenage suicide and self-injury;

															discourage the use of vaccines;

															advise phony treatments for diseases;

															advance false claims of election fraud.

															The list could continue for a while.[10] The point of it is not that the
speech environment created by Texas’s law is worse than the ones to
which the major platforms aspire on their main feeds. The point is
just that Texas’s law profoundly alters the platforms’ choices
about the views they will, and will not, convey.

															And we have time and again held that type of
regulation to interfere with protected speech. Like the editors,
cable operators, and parade organizers this Court has previously
considered, the major social-media platforms are in the business,
when curating their feeds, of combining “multifarious voices” to
create a distinctive expressive offering. Hurley, 515
U. S., at 569. The individual messages may originate with
third parties, but the larger offering is the platform’s. It is the
product of a wealth of choices about whether—and, if so, how—to
convey posts having a certain content or viewpoint. Those choices
rest on a set of beliefs about which messages are appropriate and
which are not (or which are more appropriate and which less so).
And in the aggregate they give the feed a particular expressive
quality. Consider again an opinion page editor, as in
Tornillo, who wants to publish a variety of views, but
thinks some things off-limits (or, to change the facts, worth only
a couple of column inches). “The choice of material,” the
“decisions made [as to] content,” the “treatment of public
issues”—“whether fair or unfair”—all these “constitute the exercise
of editorial control and judgment.” Tornillo, 418
U. S., at 258. For a paper, and for a platform too. And the
Texas law (like Florida’s earlier right-of-reply statute) targets
those expressive choices—in particular, by forcing the major
platforms to present and promote content on their feeds that they
regard as objectionable.

															That those platforms happily convey the lion’s
share of posts submitted to them makes no significant First
Amendment difference. Contra, 49 F. 4th, at 459–461 (arguing
otherwise). To begin with, Facebook and YouTube exclude (not to
mention, label or demote) lots of content from their News Feed and
homepage. The Community Standards and Community Guidelines set out
in copious detail the varied kinds of speech the platforms want no
truck with. And both platforms appear to put those manuals to work.
In a single quarter of 2021, Facebook removed from its News Feed
more than 25 million pieces of “hate speech content” and almost 9
million pieces of “bullying and harassment content.” App. in No.
22–555, at 80a. Similarly, YouTube deleted in one quarter more than
6 million videos violating its Guidelines. See id., at 116a.
And among those are the removals the Texas law targets. What is
more, this Court has already rightly declined to focus on the ratio
of rejected to accepted content. Recall that in Hurley, the
parade organizers welcomed pretty much everyone, excluding only
those who expressed a message of gay pride. See supra, at
18. The Court held that the organizers’ “lenient” admissions
policy—and their resulting failure to express a “particularized
message”—did “not forfeit” their right to reject the few messages
they found harmful or offensive. 515 U. S., at 569, 574. So
too here, though the excluded viewpoints differ. That Facebook and
YouTube convey a mass of messages does not license Texas to
prohibit them from deleting posts with, say, “hate speech” based on
“sexual orientation.” App. in No. 22–555, at 126a, 155a; see
id., at 431a. It is as much an editorial choice to convey
all speech except in select categories as to convey only speech
within them.

															Similarly, the major social-media platforms do
not lose their First Amendment protection just because no one will
wrongly attribute to them the views in an individual post. Contra,
49 F. 4th, at 462 (arguing otherwise). For starters, users may
well attribute to the platforms the messages that the posts convey
in toto. Those messages—communicated by the feeds as a
whole—derive largely from the platforms’ editorial decisions about
which posts to remove, label, or demote. And because that is so,
the platforms may indeed “own” the overall speech environment. In
any event, this Court has never hinged a compiler’s First Amendment
protection on the risk of misattribution. The Court did not think
in Turner—and could not have thought in Tornillo or
PG&E—that anyone would view the entity conveying the
third-party speech at issue as endorsing its content. See Turner
I, 512 U. S., at 655 (“[T]here appears little risk” of
such misattribution). Yet all those entities, the Court held, were
entitled to First Amendment protection for refusing to carry the
speech. See supra, at 14–16. To be sure, the Court noted in
PruneYard and FAIR, when denying such protection,
that there was little prospect of misattribution. See 447
U. S., at 87; 547 U. S., at 65. But the key fact in those
cases, as noted above, was that the host of the third-party speech
was not itself engaged in expression. See supra, at 16–17.
The current record suggests the opposite as to Facebook’s News Feed
and YouTube’s homepage. When the platforms use their Standards and
Guidelines to decide which third-party content those feeds will
display, or how the display will be ordered and organized, they are
making expressive choices. And because that is true, they receive
First Amendment protection.

															C

															And once that much is decided, the interest
Texas relies on cannot sustain its law. In the usual First
Amendment case, we must decide whether to apply strict or
intermediate scrutiny. But here we need not. Even assuming that the
less stringent form of First Amendment review applies, Texas’s law
does not pass. Under that standard, a law must further a
“substantial governmental interest” that is “unrelated to the
suppression of free expression.” United States v.
O’Brien, 391 U.S.
367, 377 (1968). Many possible interests relating to social
media can meet that test; nothing said here puts regulation of
NetChoice’s members off-limits as to a whole array of subjects. But
the interest Texas has asserted cannot carry the day: It is very
much related to the suppression of free expression, and it is not
valid, let alone substantial.

															Texas has never been shy, and always been
consistent, about its interest: The objective is to correct the mix
of speech that the major social-media platforms present. In this
Court, Texas described its law as “respond[ing]” to the platforms’
practice of “favoring certain viewpoints.” Brief for Texas 7; see
id., at 27 (explaining that the platforms’ “discrimination”
among messages “led to [the law’s] enactment”). The large
social-media platforms throw out (or encumber) certain messages;
Texas wants them kept in (and free from encumbrances), because it
thinks that would create a better speech balance. The current
amalgam, the State explained in earlier briefing, was “skewed” to
one side. 573 F. Supp. 3d, at 1116. And that assessment
mirrored the stated views of those who enacted the law, save that
the latter had a bit more color. The law’s main sponsor explained
that the “West Coast oligarchs” who ran social-media companies were
“silenc[ing] conservative viewpoints and ideas.” Ibid. The
Governor, in signing the legislation, echoed the point: The
companies were fomenting a “dangerous movement” to “silence”
conservatives. Id., at 1108; see id., at 1099
(“[S]ilencing conservative views is un- American, it’s un-Texan and
it’s about to be illegal in Texas”).

															But a State may not interfere with private
actors’ speech to advance its own vision of ideological balance.
States (and their citizens) are of course right to want an
expressive realm in which the public has access to a wide range of
views. That is, indeed, a fundamental aim of the First Amendment.
But the way the First Amendment achieves that goal is by preventing
the government from “tilt[ing] public debate in a preferred
direction.” Sorrell v. IMS Health Inc., 564 U.S.
552, 578–579 (2011). It is not by licensing the government to
stop private actors from speaking as they wish and
preferring some views over others. And that is so even when those
actors possess “enviable vehicle[s]” for expression. Hurley,
515 U. S., at 577. In a better world, there would be fewer
inequities in speech opportunities; and the government can take
many steps to bring that world closer. But it cannot prohibit
speech to improve or better balance the speech market. On the
spectrum of dangers to free expression, there are few greater than
allowing the government to change the speech of private actors in
order to achieve its own conception of speech nirvana. That is why
we have said in so many contexts that the government may not
“restrict the speech of some elements of our society in order to
enhance the relative voice of others.” Buckley v.
Valeo, 424 U.S.
1, 48–49 (1976) (per curiam). That unadorned interest is
not “unrelated to the suppression of free expression,” and the
government may not pursue it consistent with the First
Amendment.

															The Court’s decisions about editorial control,
as discussed earlier, make that point repeatedly. See supra,
at 18–19. Again, the question those cases had in common was whether
the government could force a private speaker, including a compiler
and curator of third-party speech, to convey views it disapproved.
And in most of those cases, the government defended its regulation
as yielding greater balance in the marketplace of ideas. But the
Court—in Tornillo, in PG&E, and again in
Hurley—held that such an interest could not support the
government’s effort to alter the speaker’s own expression. “Our
cases establish,” the PG&E Court wrote, “that the State
cannot advance some points of view by burdening the expression of
others.” 475 U. S., at 20. So the newspaper, the public
utility, the parade organizer—whether acting “fair[ly] or
unfair[ly]”—could exclude the unwanted message, free from
government interference. Tornillo, 418 U. S., at 258;
see United States Telecom Assn. v. FCC, 855 F.3d
381, 432 (CADC 2017) (Kavanaugh, J., dissenting from denial of
rehearing en banc) (“[E]xcept in rare circumstances, the First
Amendment does not allow the Government to regulate the content
choices of private editors just so that the Government may enhance
certain voices and alter the content available to the
citizenry”).[11]

															The case here is no different. The interest
Texas asserts is in changing the balance of speech on the major
platforms’ feeds, so that messages now excluded will be included.
To describe that interest, the State borrows language from this
Court’s First Amendment cases, maintaining that it is preventing
“viewpoint discrimination.” Brief for Texas 19; see supra,
at 26–27. But the Court uses that language to say what governments
cannot do: They cannot prohibit private actors from expressing
certain views. When Texas uses that language, it is to say what
private actors cannot do: They cannot decide for themselves what
views to convey. The innocent-sounding phrase does not redeem the
prohibited goal. The reason Texas is regulating the content-
moderation policies that the major platforms use for their feeds is
to change the speech that will be displayed there. Texas does not
like the way those platforms are selecting and moderating content,
and wants them to create a different expressive product,
communicating different values and priorities. But under the First
Amendment, that is a preference Texas may not impose.

															IV

															These are facial challenges, and that matters.
To succeed on its First Amendment claim, NetChoice must show that
the law at issue (whether from Texas or from Florida) “prohibits a
substantial amount of protected speech relative to its plainly
legitimate sweep.” Hansen, 599 U. S., at 770. None of
the parties below focused on that issue; nor did the Fifth or
Eleventh Circuits. But that choice, unanimous as it has been,
cannot now control. Even in the First Amendment context, facial
challenges are disfavored, and neither parties nor courts can
disregard the requisite inquiry into how a law works in all of its
applications. So on remand, each court must evaluate the full scope
of the law’s coverage. It must then decide which of the law’s
applications are constitutionally permissible and which are not,
and finally weigh the one against the other. The need for NetChoice
to carry its burden on those issues is the price of its decision to
challenge the laws as a whole.

															But there has been enough litigation already to
know that the Fifth Circuit, if it stayed the course, would get
wrong at least one significant input into the facial analysis. The
parties treated Facebook’s News Feed and YouTube’s homepage as the
heartland applications of the Texas law. At least on the current
record, the editorial judgments influencing the content of those
feeds are, contrary to the Fifth Circuit’s view, protected
expressive activity. And Texas may not interfere with those
judgments simply because it would prefer a different mix of
messages. How that matters for the requisite facial analysis is for
the Fifth Circuit to decide. But it should conduct that analysis in
keeping with two First Amendment precepts. First, presenting a
curated and “edited compilation of [third party] speech” is itself
protected speech. Hurley, 515 U. S., at 570. And
second, a State “cannot advance some points of view by burdening
the expression of others.” PG&E, 475 U. S., at 20.
To give government that power is to enable it to control the
expression of ideas, promoting those it favors and suppressing
those it does not. And that is what the First Amendment protects
all of us from.

															We accordingly vacate the judgments of the
Courts of Appeals for the Fifth and Eleventh Circuits and remand
the cases for further proceedings consistent with this opinion.

															

															It is so ordered.

Notes
1
*Justice Jackson joins Parts I,
II, and III–A of this opinion.
2
 The definition of
“social-media platforms” covers “any information service, system,
Internet search engine, or access software provider” that
“[p]rovides or enables computer access by multiple users to a
computer server, including an Internet platform or a social media
site.” Fla. Stat. §501.2041(1)(g)(1).
3
 The statute further
clarifies that it does not cover internet service providers, email
providers, and any online service, website, or app consisting
“primarily of news, sports, entertainment, or other information or
content that is not user generated but is preselected by the
provider.” §120.001(1).
4
 Although the discussion
below focuses on Texas’s content-moderation provisions, it also
bears on how the lower courts should address the
individualized-explanation provisions in the upcoming facial
inquiry. As noted, requirements of that kind violate the First
Amendment if they unduly burden expressive activity. See
Zauderer v. Office of Disciplinary Counsel of Supreme
Court of Ohio, 471 U.S.
626, 651 (1985); supra, at 11. So our explanation of why
Facebook and YouTube are engaged in expression when they make
content-moderation choices in their main feeds should inform the
courts’ further consideration of that issue.
5
 Of course, an entity
engaged in expressive activity when performing one function may not
be when carrying out another. That is one lesson of FAIR.
The Court ruled as it did because the law schools’ recruiting
services were not engaged in expression. See 547 U.S.
47, 64 (2006). The case could not have been resolved on that
ground if the regulation had affected what happened in law school
classes instead.
6
 We therefore do not deal
here with feeds whose algorithms respond solely to how users act
online—giving them the content they appear to want, without any
regard to independent content standards. See post, at 2
(Barrett, J., concurring). Like them or loathe them, the Community
Standards and Community Guidelines make a wealth of user-agnostic
judgments about what kinds of speech, including what viewpoints,
are not worthy of promotion. And those judgments show up in
Facebook’s and YouTube’s main feeds.
7
 The scope of the Texas
law, a matter crucial to the facial inquiry, is unsettled, as
previously discussed. See supra, at 10–11. The Texas
solicitor general at oral argument stated that he understood the
law to cover Facebook and YouTube, but “d[id]n’t know” whether it
also covered other platforms and applications. Tr. of Oral Arg. in
No. 22–555, pp. 61–62.
8
 In addition to barring
“censor[ship]” of “expression,” the law bars “censor[ship]” of
people. More specifically, it prohibits taking the designated
“censor[ial]” actions against any “user” based on his “viewpoint,”
regardless of whether that “viewpoint is expressed on a social
media platform.” §§143A.002(a)(1), (b); see supra, at 7.
Because the Fifth Circuit did not focus on that provision, instead
confining its analysis to the law’s ban on “censor[ing]” a “user’s
expression” on the platform, we do the same.
9
 The Texas solicitor
general explained at oral argument that the Texas law allows the
platforms to remove “categories” of speech, so long as they are not
based on viewpoint. See Tr. of Oral Arg. in No. 22–555, at 69–70;
§120.052 (Acceptable Use Policy). The example he gave was speech
about Al-Qaeda. Under the law, a platform could remove all posts
about Al-Qaeda, regardless of viewpoint. But it could not stop the
“pro-Al-Qaeda” speech alone; it would have to stop the
“anti-Al-Qaeda” speech too. Tr. of Oral Arg. in No. 22–555, at 70.
So again, the law, as described by the solicitor general, prevents
the platforms from disfavoring posts because they express one view
of a subject.
10  Details on both the enumerated
examples and similar ones are found in Facebook’s Community
Standards and YouTube’s Community Guidelines. See
https://transparency.meta.com/policies/community-standards;
https://support.google.com/youtube/answer/9288567.
11  Texas
claims Turner as a counter-example, but that decision offers
no help to speak of. Turner did indeed hold that the FCC’s
must-carry provisions, requiring cable operators to give some of
their channel space to local broadcast stations, passed First
Amendment muster. See supra, at 15. But the interest there
advanced was not to balance expressive content; rather, the
interest was to save the local-broadcast industry, so that it could
continue to serve households without cable. That interest, the
Court explained, was “unrelated to the content of expression”
disseminated by either cable or broadcast speakers. Turner
I, 512 U.S.
622, 647 (1994). And later, the Hurley Court again noted
the difference. It understood the Government interest in
Turner as one relating to competition policy: The FCC needed
to limit the cable operators’ “monopolistic,” gatekeeping position
“in order to allow for the survival of broadcasters.” 515
U. S., at 577. Unlike in regulating the parade—or here in
regulating Facebook’s News Feed or YouTube’s homepage—the
Government’s interest was “not the alteration of speech.”
Ibid. And when that is so, the prospects of permissible
regulation are entirely different.


